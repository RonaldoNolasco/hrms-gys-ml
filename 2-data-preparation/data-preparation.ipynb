{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a593920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "import datetime\n",
    "import traceback\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import jellyfish\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a8b9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "inputFolder = \"1-input\"\n",
    "processFolder = \"2-process\"\n",
    "outputFolder = \"3-output\"\n",
    "otherOutputFolder = \"../3-modelling/1-input/\"\n",
    "logsFolder = \"4-logs\"\n",
    "\n",
    "inputMainFolder = inputFolder + r\"\\main\"\n",
    "inputMastersFolder = inputFolder + r\"\\masters\"\n",
    "\n",
    "processMainFolder = processFolder + r\"\\main\"\n",
    "processSupportFolder = processFolder + r\"\\support\"\n",
    "\n",
    "dataVisualizationTopLimit = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22025f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilitarias\n",
    "def find_nth(haystack, needle, n):\n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def find_nth_right(haystack, needle, n):\n",
    "    start = haystack.rfind(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.rfind(needle, 0, start-len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def parseLineBreaksAndAccents(text):\n",
    "  return unidecode(\" \".join(text.split()))\n",
    "\n",
    "def parseNames(text):\n",
    "  return text.strip().title()\n",
    "\n",
    "def findTags(tag, color):\n",
    "  return tag.find(\"span\", {\"style\": 'font-size:10.0pt;font-family:\"Arial\",sans-serif;mso-fareast-font-family:\\n\"Times New Roman\";color:' + color })\n",
    "\n",
    "def getChildIndex(mainChildTags, title, color):\n",
    "  return next((index for index, tag in enumerate(mainChildTags) if ( parseNames(parseLineBreaksAndAccents(findTags(tag, color).text)) == title if findTags(tag, color) else False )), None)\n",
    "\n",
    "def getSectionsIndexes(mainChildTags, color):\n",
    "  sectionsIndexes = []\n",
    "  sectionsTitle = [\"Objetivo Laboral\", \"Experiencia Laboral\", \"Educacion\", \"Informatica\", \"Idiomas\", \"Otros Conocimientos\"]\n",
    "  \n",
    "  for sectionTitle in sectionsTitle:\n",
    "    sectionIndex = getChildIndex(mainChildTags, sectionTitle, color)\n",
    "    sectionsIndexes.append(sectionIndex)\n",
    "  \n",
    "  sectionsIndexes.append(len(mainChildTags)-1)\n",
    "  return sectionsIndexes\n",
    "\n",
    "def getNextSectionIndexValid(sectionsIndexes, i):\n",
    "  while(not sectionsIndexes[i]):\n",
    "    i = i + 1\n",
    "\n",
    "  return sectionsIndexes[i]\n",
    "\n",
    "def getStartAndEndIndex(sectionsIndexes, i):\n",
    "  return sectionsIndexes[i], getNextSectionIndexValid(sectionsIndexes, i+1)\n",
    "\n",
    "def readJson(path, encoding='utf-8', errors=None):\n",
    "  with open (path, \"r\", encoding=encoding, errors=errors) as f:\n",
    "    data = json.loads(f.read())\n",
    "  return data\n",
    "\n",
    "def writeJson(data, pathJson, encoding='utf-8'):\n",
    "  with open(pathJson, 'w', encoding=encoding) as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def writeCsv(data, pathCsv, encoding='utf-8'):\n",
    "  with open(pathCsv, 'w', newline='', encoding=encoding) as f:\n",
    "    if data:\n",
    "      writer = csv.DictWriter(f, fieldnames=data[0].keys(), lineterminator='\\n')\n",
    "      writer.writeheader()\n",
    "      writer.writerows(data)\n",
    "    else:\n",
    "      f.write(\"\")\n",
    "\n",
    "def writeTxt(data, pathTxt, encoding='utf-8'):\n",
    "  with open(pathTxt, 'w', encoding=encoding) as f:\n",
    "    f.write(data)\n",
    "\n",
    "def readAndWriteSupport(folder, file, writeFileName, readEncoding = 'utf-8', writeEncoding = 'utf-8'):\n",
    "  with open(os.path.join(folder, file), \"r\", encoding=readEncoding) as file:\n",
    "    data = [{k: parseNames(v) for k, v in row.items()} for row in csv.DictReader(file, skipinitialspace=True)]\n",
    "\n",
    "  with open(os.path.join(processSupportFolder, writeFileName), 'w', encoding=writeEncoding) as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "  \n",
    "  return data\n",
    "\n",
    "def readAndWriteInHome(folder, writeFileName):\n",
    "  folderFiles = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "  attendees = []\n",
    "  \n",
    "  for file in folderFiles:\n",
    "    # Detectar si tienen el To: en el texto\n",
    "    try:\n",
    "      # Detectar el encoding\n",
    "      encoding = \"utf-16\"\n",
    "      with open(file, \"r\") as f:\n",
    "        encoding = 'windows-1252' if \"charset=windows-1252\" in f.read() else 'utf-16'\n",
    "\n",
    "      # El .read() manda el cursor al final del archivo, por eso si se usa 2 veces, la 2da vez no encuentra nada\n",
    "      with open(file, \"r\", encoding=encoding) as f:\n",
    "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "        #print(str(soup))\n",
    "        if '<span style=\"color:black\">To:<span style=\"mso-tab-count:1\">' in parseLineBreaksAndAccents(str(soup)):\n",
    "          emailAttendeesText = parseLineBreaksAndAccents(soup.find_all(\"span\", {\"style\": 'color:black'})[5].text)\n",
    "          emailAttendeesList = emailAttendeesText.split(\"; \")\n",
    "          for emailAttendee in emailAttendeesList:\n",
    "            if emailAttendee not in attendees:\n",
    "              attendees.append(emailAttendee)\n",
    "    except Exception as e:\n",
    "      print(file)\n",
    "      traceback.print_exc()\n",
    "      print()\n",
    "      pass\n",
    "\n",
    "  data = [ {\"name\": parseNames(participant)} for participant in attendees ]\n",
    "\n",
    "  with open(os.path.join(processSupportFolder, writeFileName), 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "  return data\n",
    "\n",
    "def getCompare(data, fields):\n",
    "  return (list(set([\" \".join(x for x in [elem[y] for y in fields] if x) for elem in data if elem])))\n",
    "\n",
    "def readSupport(file, readEncoding = 'utf-8'):\n",
    "  with open(os.path.join(file), \"r\", encoding=readEncoding) as file:\n",
    "    data = [{k: parseNames(v) for k, v in row.items()} for row in csv.DictReader(file, skipinitialspace=True)]\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88d18df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAndWritePreprocessedData(preprocessedData, masters):\n",
    "  ## Decisiones propias para el modelo\n",
    "  # Transformar la data a dataframe\n",
    "  df = pd.DataFrame(preprocessedData)\n",
    "\n",
    "  # Eliminar los campos que ya no se usarán\n",
    "  columnsToDelete = [\n",
    "    \"candidatePostulationDate\", \"jobId\", \"candidateFullName\", \"candidateDocumentNumber\", \"candidateBirthDate\", \"candidateAddress\", \"candidateHomeNumber\", \"candidateCellphoneNumber\", \"candidateEmail\", \"workObjetive\", \"lastWorkDays\", \"lastWorkDescription\", \"lastEducationDays\"\n",
    "  ]\n",
    "  df = df.drop(columns=columnsToDelete)\n",
    "\n",
    "  # Aplicar las equivalencias a los campos: jobProfileName, lastWorkCompany, lastWorkArea, lastWorkName, lastEducationCompany, lastEducationArea, lastEducationName, lastEducationName\n",
    "\n",
    "  ## Decisiones por la metodología\n",
    "\n",
    "  #print(df.columns)\n",
    "\n",
    "  # Analisis de valores nulos\n",
    "  # Reemplazando las cadenas vacias a nulos\n",
    "  df = df.replace(r'^\\s*$', None, regex=True)\n",
    "\n",
    "  # Eliminando columnas con al menos un nulo\n",
    "  df = df.dropna()\n",
    "\n",
    "  # Obteniendo el porcentaje de nulos por columna\n",
    "  percentMissing = df.isnull().sum() * 100 / len(df)\n",
    "  missingValueDf = pd.DataFrame({'columnName': df.columns, 'percentMissing': percentMissing })\n",
    "  missingValueDf = missingValueDf.sort_values('percentMissing', ascending=False)\n",
    "\n",
    "  print(missingValueDf)\n",
    "\n",
    "  # Al ninguno superar el 30% de nulos, no se eliminará ninguna columna\n",
    "\n",
    "\n",
    "\n",
    "  \"\"\"df = df.replace(r'^\\s*$', None, regex=True)\n",
    "\n",
    "  percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "  missing_value_df = pd.DataFrame({'column_name': df.columns, 'percent_missing': percent_missing })\n",
    "  missing_value_df.sort_values('percent_missing', ascending=False, inplace=True)\n",
    "  \n",
    "  print(missing_value_df)\"\"\"\n",
    "  \"\"\"# Añadiendo los nuevos campos del puesto de trabajo\n",
    "  for elem in preprocessedData:\n",
    "    for elem2 in masters[0]:\n",
    "      if elem[\"profileName\"] == elem2[\"profileName\"]:\n",
    "        elem[\"profileName\"] = elem2[\"equivalence\"]\n",
    "\n",
    "  # Añadiendo los nuevos campos del ultimo trabajo\n",
    "  for elem in preprocessedData:\n",
    "    for elem2 in masters[1]:\n",
    "      if elem[\"lastWorkCenter\"] == elem2[\"lastWorkCenter\"]:\n",
    "        elem[\"lastWorkCenter\"] = elem2[\"equivalence\"]\n",
    "\n",
    "  # Añadiendo los nuevos campos de la ultima posición\n",
    "  for elem in preprocessedData:\n",
    "    for elem2 in masters[2]:\n",
    "      if elem[\"lastWorkPosition\"] == elem2[\"lastWorkPosition\"]:\n",
    "        elem[\"lastWorkPosition\"] = elem2[\"equivalence\"]\n",
    "\n",
    "  # Añadiendo los nuevos campos del ultimo trabajo\n",
    "  for elem in preprocessedData:\n",
    "    for elem2 in masters[3]:\n",
    "      if elem[\"studyCenter\"] == elem2[\"studyCenter\"]:\n",
    "        elem[\"studyCenter\"] = elem2[\"equivalence\"]\"\"\"\n",
    "\n",
    "  # Regresando el dataframe a array de dicts\n",
    "  preprocessedData = df.to_dict('records')\n",
    "\n",
    "  writeJson(preprocessedData, os.path.join(outputFolder, 'result.json'), 'utf-8')\n",
    "  writeCsv(preprocessedData, os.path.join(outputFolder, 'result.csv'), 'utf-8')\n",
    "\n",
    "  writeJson(preprocessedData, os.path.join(otherOutputFolder, 'result.json'), 'utf-8')\n",
    "  writeCsv(preprocessedData, os.path.join(otherOutputFolder, 'result.csv'), 'utf-8')\n",
    "\n",
    "  return preprocessedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1994b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeData(preprocessedData):\n",
    "  df = pd.DataFrame(preprocessedData)\n",
    "\n",
    "  print(df.count())\n",
    "\n",
    "  columns = [elem for elem in df.columns]\n",
    "\n",
    "  for column in columns:\n",
    "    topDf = df[column].value_counts()[:dataVisualizationTopLimit]\n",
    "    print(topDf)\n",
    "    y_axis = list(reversed(topDf.index))\n",
    "    x_axis = list(reversed(topDf.values))\n",
    "    plt.ylabel(column)\n",
    "    plt.barh(y_axis, x_axis)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9125a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  # Definiendo el inicio del proceso\n",
    "  startTime = datetime.datetime.now()\n",
    "  print(\"Inicio: \" + str(startTime))\n",
    "  print(\"Se inició el procesamiento\")\n",
    "\n",
    "  # Leyendo la data obtenida en el entendimiento de los datos\n",
    "  bumeranData = readJson(os.path.join(inputMainFolder, 'result.json'))\n",
    "\n",
    "  # Leyendo archivos maestros\n",
    "  profileNameMaster = readSupport(os.path.join(inputMastersFolder, 'profileName.csv'))\n",
    "  lastWorkCenterMaster = readSupport(os.path.join(inputMastersFolder, 'lastWorkCenter.csv'))\n",
    "  lastWorkPositionMaster = readSupport(os.path.join(inputMastersFolder, 'lastWorkPosition.csv'))\n",
    "  studyCenterMasterMaster = readSupport(os.path.join(inputMastersFolder, 'studyCenter.csv'))\n",
    "\n",
    "  isPreprocessed = False\n",
    "\n",
    "  # Aplicando los datos de los maestros y validaciones\n",
    "  preprocessedData = readJson(os.path.join(outputFolder, 'result.json')) if isPreprocessed else readAndWritePreprocessedData(bumeranData, [profileNameMaster, lastWorkCenterMaster, lastWorkPositionMaster, studyCenterMasterMaster])\n",
    "  print(\"Se terminó el preprocesamiento\")\n",
    "\n",
    "  #visualizeData(preprocessedData)\n",
    "\n",
    "  # Definiendo el fin del proceso\n",
    "  endTime = datetime.datetime.now()\n",
    "  print(\"Fin: \" + str(endTime))\n",
    "  print(\"Tiempo: \" + str(endTime-startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0eaed034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio: 2023-05-21 14:25:54.929446\n",
      "Se inició el procesamiento\n",
      "                                          columnName  percentMissing\n",
      "jobProfileName                        jobProfileName             0.0\n",
      "lastEducationCountry            lastEducationCountry             0.0\n",
      "otherSkills                              otherSkills             0.0\n",
      "languages                                  languages             0.0\n",
      "technicalSkills                      technicalSkills             0.0\n",
      "studiesNumber                          studiesNumber             0.0\n",
      "yearsOfStudy                            yearsOfStudy             0.0\n",
      "lastEducationDegree              lastEducationDegree             0.0\n",
      "lastEducationStatus              lastEducationStatus             0.0\n",
      "lastEducationName                  lastEducationName             0.0\n",
      "lastEducationArea                  lastEducationArea             0.0\n",
      "lastEducationCompany            lastEducationCompany             0.0\n",
      "candidateResidenceCountry  candidateResidenceCountry             0.0\n",
      "worksNumber                              worksNumber             0.0\n",
      "yearsOfExperience                  yearsOfExperience             0.0\n",
      "lastWorkName                            lastWorkName             0.0\n",
      "lastWorkArea                            lastWorkArea             0.0\n",
      "lastWorkCountry                      lastWorkCountry             0.0\n",
      "lastWorkCompany                      lastWorkCompany             0.0\n",
      "salary                                        salary             0.0\n",
      "candidateBirthCountry          candidateBirthCountry             0.0\n",
      "candidateCivilStatus            candidateCivilStatus             0.0\n",
      "hired                                          hired             0.0\n",
      "Se terminó el preprocesamiento\n",
      "Fin: 2023-05-21 14:25:55.754379\n",
      "Tiempo: 0:00:00.824933\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
