{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6793a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "import datetime\n",
    "import traceback\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import jellyfish\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from threading import Thread\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125e31d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "sourceDataFolder = \"1-source-data\"\n",
    "\n",
    "bumeraniterationNumber = \"5\"\n",
    "bumeranRootPath = sourceDataFolder + r\"\\main\\bumeran\\iteration-\" + bumeraniterationNumber\n",
    "\n",
    "intermFilesFolder = \"2-intermediate-files\"\n",
    "mergedMainFolder = \"3-results\"\n",
    "logsFolder = \"4-logs\"\n",
    "\n",
    "dataVisualizationTopLimit = 20\n",
    "\n",
    "counterIn = 0\n",
    "counterOut = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85e41e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilitarias\n",
    "def find_nth(haystack, needle, n):\n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def find_nth_right(haystack, needle, n):\n",
    "    start = haystack.rfind(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.rfind(needle, 0, start-len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def parseLineBreaksAndAccents(text):\n",
    "  return unidecode(\" \".join(text.split()))\n",
    "\n",
    "def parseNames(text):\n",
    "  return text.strip().title()\n",
    "\n",
    "def findTags(tag, color):\n",
    "  return tag.find(\"span\", {\"style\": 'font-size:10.0pt;font-family:\"Arial\",sans-serif;mso-fareast-font-family:\\n\"Times New Roman\";color:' + color })\n",
    "\n",
    "def getChildIndex(mainChildTags, title, color):\n",
    "  return next((index for index, tag in enumerate(mainChildTags) if ( findTags(tag, color).text == title if findTags(tag, color) else False )), None)\n",
    "\n",
    "def getSectionsIndexes(mainChildTags, color):\n",
    "  sectionsIndexes = []\n",
    "  sectionsTitle = [\"Experiencia laboral\", \"Educación\", \"Informática\", \"Idiomas\", \"Otros Conocimientos\"]\n",
    "  \n",
    "  for sectionTitle in sectionsTitle:\n",
    "    sectionIndex = getChildIndex(mainChildTags, sectionTitle, color)\n",
    "    sectionsIndexes.append(sectionIndex)\n",
    "  \n",
    "  sectionsIndexes.append(len(mainChildTags)-1)\n",
    "  return sectionsIndexes\n",
    "\n",
    "def getNextSectionIndexValid(sectionsIndexes, i):\n",
    "  while(not sectionsIndexes[i]):\n",
    "    i = i + 1\n",
    "\n",
    "  return sectionsIndexes[i]\n",
    "\n",
    "def getStartAndEndIndex(sectionsIndexes, i):\n",
    "  return sectionsIndexes[i], getNextSectionIndexValid(sectionsIndexes, i+1)\n",
    "\n",
    "def readJson(path, encoding='utf-8', errors=None):\n",
    "  with open (path, \"r\", encoding=encoding, errors=errors) as f:\n",
    "    data = json.loads(f.read())\n",
    "  return data\n",
    "\n",
    "def writeJson(data, pathJson, encoding='utf-8'):\n",
    "  with open(pathJson, 'w', encoding=encoding) as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def writeCsv(data, pathCsv, encoding='utf-8'):\n",
    "  with open(pathCsv, 'w', newline='', encoding=encoding) as f:\n",
    "    if data:\n",
    "      writer = csv.DictWriter(f, fieldnames=data[0].keys(), lineterminator='\\n')\n",
    "      writer.writeheader()\n",
    "      writer.writerows(data)\n",
    "    else:\n",
    "      f.write(\"\")\n",
    "\n",
    "def writeTxt(data, pathTxt, encoding='utf-8'):\n",
    "  with open(pathTxt, 'w', encoding=encoding) as f:\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd47f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones principales\n",
    "def getFiles(rootPath, source):\n",
    "  # Obteniendo todas las carpetas de los perfiles\n",
    "  folders = [f for f in os.listdir(rootPath) if os.path.isdir(os.path.join(rootPath, f))]\n",
    "\n",
    "  # Definiendo la lista de archivos final\n",
    "  files = []\n",
    "\n",
    "  # Iterando sobre las carpetas\n",
    "  for folder in folders:\n",
    "    # Obteniendo los archivos por cada carpeta\n",
    "    folderFiles = [os.path.join(rootPath, folder, f) for f in os.listdir(os.path.join(rootPath, folder)) if os.path.isfile(os.path.join(rootPath, folder, f))]\n",
    "\n",
    "    # Agregando esos archivos a la lista de archivos final\n",
    "    if folderFiles:\n",
    "      files.extend(folderFiles)\n",
    "\n",
    "  print(\"Archivos originales: \" + str(len(files)))\n",
    "\n",
    "  # Filtros específicos por cada origen\n",
    "  if source == \"bumeran\":\n",
    "    filesFiltered = [f for f in files if (\"\\Has recibido un CV para el aviso\" in f)]\n",
    "  else:\n",
    "    filesFiltered = files\n",
    "\n",
    "  print(\"Archivos filtrados solo con postulaciones: \" + str(len(filesFiltered)))\n",
    "\n",
    "  # Filtros para considerar un archivo especifico\n",
    "  filterIsIn = [\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\analista-calidad\\Has recibido un CV para el aviso _Analista de Calidad_(76).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\analista-funcional\\Has recibido un CV para el aviso _Analista Funcional_(110).html\"\n",
    "    #r\"1-source-data\\main\\linkedin\\iteration-2\\bandeja-entrada\\New application_ .NET Developer from Cesar Ospino.html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\a-considerar\\Has recibido un CV para el aviso _Practicante de Infrastructure & Cloud_(1).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\a-considerar\\Has recibido un CV para el aviso _Practicante de Infrastructure & Cloud_.html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\analista-calidad\\Has recibido un CV para el aviso _Analista de Calidad Sr. (Test Lead)_(1).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\analista-calidad\\Has recibido un CV para el aviso _Analista de Calidad_(1).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\bandeja-entrada\\Has recibido un CV para el aviso _Analista Programador Senior_Full Stack_(2).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\bandeja-entrada\\Has recibido un CV para el aviso _Analista Programador_(6).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\programador-net\\Has recibido un CV para el aviso _Programador .Net Senior_(4).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\bandeja-entrada\\Has recibido un CV para el aviso _Analista de Sistemas_(24).html\"\n",
    "    # 5 casos con estudios en blanco\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\bandeja-entrada\\Has recibido un CV para el aviso _Especialista en Analítica Jr_(50).html\",\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\bandeja-entrada\\Has recibido un CV para el aviso _Gestor de Servicios de TI_(132).html\",\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\bandeja-entrada-asistente-social\\Has recibido un CV para el aviso _Asistente Social_(173).html\",\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\bandeja-entrada-asistente-social\\Has recibido un CV para el aviso _Asistente Social_(71).html\",\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\ejecutivo-comercial\\Has recibido un CV para el aviso _Ejecutivo Comercial TI_(38).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\bandeja-entrada-asistente-social\\Has recibido un CV para el aviso _Asistente Social_(196).html\"\n",
    "  ]\n",
    "  filesFilteredIsIn = [f for f in filesFiltered if ( f in filterIsIn if filterIsIn else True )]\n",
    "\n",
    "  # Filtros para omitir un archivo especifico\n",
    "  filterIsNotIn = [\n",
    "    # 3 archivos con data incompleta (se mantiene en iteracion 4 y 5)\n",
    "    r\"1-source-data\\main\\bumeran\\iteration-5\\bandeja-entrada-2022\\Has recibido un CV para el aviso _Analista Programador Senior_Full Stack_(2).html\",\n",
    "    r\"1-source-data\\main\\bumeran\\iteration-5\\bandeja-entrada-2022\\Has recibido un CV para el aviso _Analista Programador_(6).html\",\n",
    "    r\"1-source-data\\main\\bumeran\\iteration-5\\programador-net\\Has recibido un CV para el aviso _Programador .Net Senior_(4).html\",\n",
    "  ]\n",
    "\n",
    "  filesFilteredNotIn = [f for f in filesFilteredIsIn if ( f not in filterIsNotIn if filterIsNotIn else True )]\n",
    "\n",
    "  print(\"Archivos filtrados solo con data completa: \" + str(len(filesFilteredNotIn)))\n",
    "\n",
    "  # Limites superiores e inferiores en la búsqueda de archivos\n",
    "  startLimit = None\n",
    "  topLimit = 1000\n",
    "  filesFilteredLimits = filesFilteredNotIn[(startLimit-1 if startLimit else 0): (topLimit if topLimit else len(files))]\n",
    "\n",
    "  return filesFilteredLimits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21425e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEncodingBumeran(file):\n",
    "  stringLog = \"\"\n",
    "\n",
    "  encoding = \"windows-1252\"\n",
    "  try:\n",
    "    with open(file, \"r\", encoding=\"utf-16-le\") as f:\n",
    "      if \"charset=unicode\" in f.read():\n",
    "        encoding = 'utf-16-le'\n",
    "      else:\n",
    "        raise Exception\n",
    "  except Exception as e:\n",
    "    try:\n",
    "      with open(file, \"r\", encoding=\"windows-1252\") as f:\n",
    "        if \"charset=windows-1252\" in f.read():\n",
    "          encoding = 'windows-1252'\n",
    "        else:\n",
    "          encoding = encoding\n",
    "    except Exception as e:\n",
    "      print(file)\n",
    "      stringLog = stringLog + file + \"\\n\"\n",
    "      traceback.print_exc()\n",
    "      stringLog = stringLog + traceback.format_exc() + \"\\n\"\n",
    "      print()\n",
    "      stringLog = stringLog + \"\\n\"\n",
    "      pass\n",
    "      \n",
    "  return encoding, stringLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d1455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCandidateBumeran(file, encoding, stringLog):\n",
    "  candidateData = {}\n",
    " \n",
    "  try:\n",
    "  # Abriendo el archivo\n",
    "    with open(file, \"r\", encoding=encoding) as myFile:\n",
    "      #Parseando el archivo html a soup\n",
    "      soup = BeautifulSoup(myFile.read(), \"lxml\")\n",
    "\n",
    "      # Obteniendo cada uno de los campos para la data\n",
    "\n",
    "      # Fecha de postulación\n",
    "      spanDates = soup.find_all(\"span\", {\"style\": \"color:black\"})\n",
    "      if len(spanDates) > 4:\n",
    "        rawPostulationDate = spanDates[3].text\n",
    "        postulationDate = datetime.datetime.strptime(spanDates[3].text, \"%A, %B %d, %Y %I:%M %p\")\n",
    "      else:\n",
    "        spanDates2 = soup.find_all(\"span\", {\"style\": 'font-family:\"Calibri\",sans-serif;color:black'})\n",
    "        rawPostulationDate = spanDates2[3].text\n",
    "        postulationDate = datetime.datetime.strptime(spanDates2[3].text, \"%A, %B %d, %Y %I:%M %p\")\n",
    "        \n",
    "      candidateData[\"candidatePostulationDate\"] = postulationDate.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "      # Definiendo los colores a usar para las búsquedas según las fechas\n",
    "      # En la fecha 2020-06-11 cambio el color de los titulos de cada sección\n",
    "      inflexionDate = datetime.datetime(2020,6,11,12,5,0)\n",
    "      colorTitle, colorSections = (\"#008599\", \"#2192C9\") if postulationDate <= inflexionDate else (\"#E90066\", \"#0A26EE\")\n",
    "\n",
    "      # Nombre del perfil\n",
    "      #candidateData[\"jobProfileName\"] = parseNames(parseLineBreaksAndAccents(soup.findAll(\"span\", {\"style\": (\"color:\" + colorTitle) })[0].text))\n",
    "\n",
    "      liTags = soup.find_all(\"ul\", {\"type\": \"disc\"})[0].find_all(\"li\")\n",
    "\n",
    "      # Nombre del candidato\n",
    "      firstLinePased = parseNames(parseLineBreaksAndAccents(liTags[0].text))\n",
    "      candidateData[\"candidateFullName\"] = firstLinePased\n",
    "      \n",
    "      \"\"\"\n",
    "      expectedCivilStatusValues = [\"Soltero/A\", \"Casado/A\", \"Divorciado/A\", \"Pareja De Hecho\", \"Viudo/A\", \"Union Libre\"]\n",
    "      secondLineParsed = parseNames(parseLineBreaksAndAccents(liTags[1].text))\n",
    "      candidateData[\"secondLineParsed\"] = secondLineParsed\n",
    "      \n",
    "      # Pais de residencia\n",
    "      firstcommaIndex = secondLineParsed.find(\",\")\n",
    "      if firstcommaIndex != -1:\n",
    "        candidateData[\"candidateResidenceCountry\"] = secondLineParsed[:firstcommaIndex]\n",
    "      else:\n",
    "        candidateData[\"candidateResidenceCountry\"] = \"\"\n",
    "\n",
    "      # Estado civil\n",
    "      civilStatus = \"\"\n",
    "      for value in expectedCivilStatusValues:\n",
    "        if value in secondLineParsed:\n",
    "          civilStatus = value\n",
    "      candidateData[\"candidateCivilStatus\"] = civilStatus\n",
    "\n",
    "      # Numero de documento\n",
    "      # Propiedad especial del rfind, el -1 coincide con el inicio del string, por lo que es lo mismo tomarlo con o sin el indice, en caso el index no sea -1\n",
    "      lastSpaceIndex = secondLineParsed.rfind(\" \")\n",
    "      candidateData[\"candidateDocumentNumber\"] = re.sub(\"[^0-9]\", \"\", secondLineParsed[lastSpaceIndex+1:]) \"\"\"\n",
    "\n",
    "      \"\"\"# Fecha de nacimiento\n",
    "      thirdLineParsed = parseNames(parseLineBreaksAndAccents(liTags[2].text))\n",
    "      birthDate = thirdLineParsed[thirdLineParsed.find(\":\")+2:]\n",
    "      if birthDate != \"\":\n",
    "        candidateData[\"candidateBirthDate\"] = datetime.datetime.strptime(thirdLineParsed[thirdLineParsed.find(\":\")+2:], \"%d-%m-%Y\").strftime(\"%Y-%m-%d\")\n",
    "      else:\n",
    "        candidateData[\"candidateBirthDate\"] = \"\"\"\"\"\n",
    "\n",
    "      \"\"\"# Pais de nacimiento\n",
    "      fourthLineParsed = parseNames(parseLineBreaksAndAccents(liTags[3].text))\n",
    "      candidateData[\"candidateBirthCountry\"] = fourthLineParsed[fourthLineParsed.find(\":\")+2:]\"\"\"\n",
    "\n",
    "      # Dirección\n",
    "      fifthLineParsed = parseNames(parseLineBreaksAndAccents(liTags[4].text))\n",
    "      candidateData[\"candidateAddress\"] = fifthLineParsed\n",
    "      candidateData[\"len\"] = fifthLineParsed.count(\",\")\n",
    "       \n",
    "      if fifthLineParsed.count(\",\") == 1:\n",
    "        candidateData[\"addressCountry\"] = fifthLineParsed[find_nth_right(fifthLineParsed, \",\", 1)+2:]\n",
    "        candidateData[\"addressDepartment\"] = \"\"\n",
    "        candidateData[\"addressDistrict\"] = \"\"\n",
    "        candidateData[\"address\"] = fifthLineParsed[:find_nth_right(fifthLineParsed, \",\", 1)]\n",
    "      elif fifthLineParsed.count(\",\") == 2:\n",
    "        candidateData[\"addressCountry\"] = fifthLineParsed[find_nth_right(fifthLineParsed, \",\", 1)+2:]\n",
    "        candidateData[\"addressDepartment\"] = \"\"\n",
    "        candidateData[\"addressDistrict\"] = fifthLineParsed[find_nth_right(fifthLineParsed, \",\", 2)+2:find_nth_right(fifthLineParsed, \",\", 1)]\n",
    "        candidateData[\"address\"] = fifthLineParsed[:find_nth_right(fifthLineParsed, \",\", 2)]\n",
    "      elif fifthLineParsed.count(\",\") >= 3:\n",
    "        candidateData[\"addressCountry\"] = fifthLineParsed[find_nth_right(fifthLineParsed, \",\", 1)+2:]\n",
    "        candidateData[\"addressDepartment\"] = fifthLineParsed[find_nth_right(fifthLineParsed, \",\", 2)+2:find_nth_right(fifthLineParsed, \",\", 1)]\n",
    "        candidateData[\"addressDistrict\"] = fifthLineParsed[find_nth_right(fifthLineParsed, \",\", 3)+2:find_nth_right(fifthLineParsed, \",\", 2)]\n",
    "        candidateData[\"address\"] = fifthLineParsed[:find_nth_right(fifthLineParsed, \",\", 3)]\n",
    "      else:\n",
    "        candidateData[\"addressCountry\"] = \"\"\n",
    "        candidateData[\"addressDepartment\"] = \"\"\n",
    "        candidateData[\"addressDistrict\"] = \"\"\n",
    "        candidateData[\"address\"] = \"\"\n",
    "\n",
    "\n",
    "      \n",
    "      # Nombre del postulante\n",
    "      \"\"\"rawcandidateName = [0].find_all(\"span\")[0].text # El split join tambien quita saltos de línea\n",
    "      candidateData[\"candidateName\"] = parseNames(parseLineBreaksAndAccents(rawcandidateName))\n",
    "\n",
    "      candidateData[\"contactData\"] = str(soup.find_all(\"ul\", {\"type\": \"disc\"})[0])\"\"\"\n",
    "\n",
    "      # Pais de residencia\n",
    "      \"\"\"tagResidenceCountry = soup.find_all(\"ul\", {\"type\": \"disc\"})[0].find_all(\"span\")\n",
    "      if len(tagResidenceCountry) > 2:\n",
    "        if tagResidenceCountry[2].text.find(\",\") != -1:\n",
    "          residenceCountry = parseNames(parseLineBreaksAndAccents(tagResidenceCountry[2].text[0:tagResidenceCountry[2].text.find(\",\")]))\n",
    "          candidateData[\"residenceCountry\"] = residenceCountry\n",
    "          if residenceCountry == \"\":\n",
    "            raise Exception(\"Error no mapeado 3\")\n",
    "        else:\n",
    "          raise Exception(\"Error no mapeado 2\")\n",
    "      else:\n",
    "        raise Exception(\"Error no mapeado 1 (posiblemente data incompleta como los 3 casos)\")\n",
    "      \n",
    "      \n",
    "      mainDivTag = soup.find_all(\"div\", {\"style\": 'background-position-x:50%;background-position-y:100%'})[0]\n",
    "      mainChildTags = mainDivTag.find_all(recursive=False)\n",
    "\n",
    "      sectionsIndexes = getSectionsIndexes(mainChildTags, colorSections)\n",
    "      \n",
    "      # Experiencia laboral\n",
    "      startIndex, endIndex = getStartAndEndIndex(sectionsIndexes, 0)\n",
    "\n",
    "      if startIndex and endIndex:\n",
    "        workExperienceTags = mainChildTags[startIndex+2:endIndex]\n",
    "        candidateData[\"lastWorkCenter\"] = parseNames(parseLineBreaksAndAccents(workExperienceTags[0].find_all(\"b\")[0].text))\n",
    "        candidateData[\"lastWorkPosition\"] = parseNames(parseLineBreaksAndAccents(workExperienceTags[1].find_all(\"b\")[0].text))\n",
    "\n",
    "        daysOfExperience = 0\n",
    "        for index in range(0, len(workExperienceTags), 2):\n",
    "          startDate = datetime.datetime.strptime(parseLineBreaksAndAccents(workExperienceTags[index].text)[0: parseLineBreaksAndAccents(workExperienceTags[index].text).find(\" - \")] , \"%d-%m-%Y\")\n",
    "          endDateText = parseLineBreaksAndAccents(workExperienceTags[index].text)[parseLineBreaksAndAccents(workExperienceTags[index].text).find(\" - \")+3: parseLineBreaksAndAccents(workExperienceTags[index].text).find(\" | \")]\n",
    "          endDate = datetime.datetime.strptime(endDateText, \"%d-%m-%Y\") if endDateText != \"Presente\" else datetime.datetime.strptime(rawPostulationDate, \"%A, %B %d, %Y %I:%M %p\")\n",
    "          daysOfExperience = daysOfExperience + (endDate - startDate).days\n",
    "        candidateData[\"yearsOfExperience\"] = int(daysOfExperience/365)\n",
    "        candidateData[\"worksNumber\"] = int(len(workExperienceTags)/2)\n",
    "\n",
    "        if (candidateData[\"lastWorkCenter\"] == \"\" or candidateData[\"lastWorkPosition\"] == \"\"):\n",
    "          raise Exception(\"Error no mapeado (experiencia laboral)\")\n",
    "      else:\n",
    "        raise Exception(\"561 errores mapeados (experiencia laboral)\")\n",
    "      \n",
    "      \n",
    "      # Carrera profesional (última alcanzada)\n",
    "      startIndex, endIndex = getStartAndEndIndex(sectionsIndexes, 1)\n",
    "\n",
    "      if startIndex and endIndex:\n",
    "        educationTags = mainChildTags[startIndex+2:endIndex]\n",
    "        \n",
    "        careerTags0 = educationTags[0].find_all(\"b\")\n",
    "        candidateData[\"studyCenter\"] = parseNames(parseLineBreaksAndAccents(careerTags0[0].text))\n",
    "        candidateData[\"careerField\"] = parseNames(parseLineBreaksAndAccents(careerTags0[1].text))\n",
    "\n",
    "        careerTags1 = educationTags[1].find_all(\"span\")\n",
    "        tempText = careerTags1[len(careerTags1)-2].text\n",
    "        candidateData[\"careerStatus\"] = parseNames(parseLineBreaksAndAccents(tempText[tempText.find(\",\")+2:tempText.rfind(\",\")]))\n",
    "        candidateData[\"careerDegree\"] = parseNames(parseLineBreaksAndAccents(tempText[tempText.rfind(\",\")+2:tempText.rfind(\".\")]))\n",
    "        candidateData[\"studiesNumber\"] = int(len(educationTags)/2)\n",
    "\n",
    "        if (candidateData[\"studyCenter\"] == \"\" or candidateData[\"careerField\"] == \"\" or candidateData[\"careerStatus\"] == \"\" or candidateData[\"careerDegree\"] == \"\"):\n",
    "          raise Exception(\"Error no mapeado (educación)\")\n",
    "      else:\n",
    "        raise Exception(\"121 errores mapeados (educación)\")\n",
    "\n",
    "      \n",
    "      # Habilidades técnicas\n",
    "      startIndex, endIndex = getStartAndEndIndex(sectionsIndexes, 2)\n",
    "\n",
    "      if startIndex and endIndex:\n",
    "        # Falta mejorar la validación (algunos tienen solo u, solo carrera o ninguno)\n",
    "        technicalSkillsTags = mainChildTags[startIndex+2:endIndex][0].find_all(\"span\")\n",
    "        candidateData[\"technicalSkills\"] = int(len(technicalSkillsTags)/4)\n",
    "      else:\n",
    "        raise Exception(\"Errores no mapeados (habilidades técnicas)\")\n",
    "\n",
    "      # Lenguajes\n",
    "      startIndex, endIndex = getStartAndEndIndex(sectionsIndexes, 3)\n",
    "\n",
    "      if startIndex and endIndex:\n",
    "        # Falta mejorar la validación (algunos tienen solo u, solo carrera o ninguno)\n",
    "        languagesTags = mainChildTags[startIndex+2:endIndex][0].find_all(\"span\")\n",
    "        candidateData[\"languages\"] = int(len(languagesTags)/7)\n",
    "      else:\n",
    "        raise Exception(\"Errores no mapeados (lenguajes)\")\n",
    "\n",
    "      # Otros conocimientos (habilidades blandas)\n",
    "      startIndex, endIndex = getStartAndEndIndex(sectionsIndexes, 4)\n",
    "\n",
    "      if startIndex and endIndex:\n",
    "        # Falta mejorar la validación (algunos tienen solo u, solo carrera o ninguno)\n",
    "        otherKnowledgesTags = mainChildTags[startIndex+2:endIndex][0].find_all(\"span\")\n",
    "        candidateData[\"anotherSkills\"] = int(len(otherKnowledgesTags)/3)\n",
    "      else:\n",
    "        raise Exception(\"Errores no mapeados (otras habilidades)\")\n",
    "\n",
    "      # Pendiente de analizar casos fraccionarios (se trunco para caso práctico)\n",
    "\n",
    "      # Salario pretendido\n",
    "      tagsSalary = [index for index, tag in enumerate(soup.find_all(\"span\")) if \"Sueldo pretendido\" in tag.text]\n",
    "      if len(tagsSalary) > 0:\n",
    "        rawSalary = soup.find_all(\"span\")[tagsSalary[0]+1].text\n",
    "        candidateData[\"salary\"] = int(rawSalary[rawSalary.find(\"$\")+1:rawSalary.find(\".\")])\n",
    "      else:\n",
    "        raise Exception(\"Errores no mapeados (salario)\")\n",
    "        \"\"\"\n",
    "\n",
    "  except Exception as e:\n",
    "    candidateData = {}\n",
    "    #print(file)\n",
    "    stringLog = stringLog + str(file) + \"\\n\"\n",
    "    #print(traceback.format_exc())\n",
    "    stringLog = stringLog + traceback.format_exc() + \"\\n\"\n",
    "    #print()\n",
    "    stringLog = stringLog + \"\\n\"\n",
    "    pass\n",
    "\n",
    "  return candidateData, stringLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ec7e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(Thread):\n",
    "  def __init__(self, queue):\n",
    "    Thread.__init__(self)\n",
    "    self.queue = queue\n",
    "    self.results = []\n",
    "\n",
    "  def run(self):\n",
    "    while True:\n",
    "      file = self.queue.get()\n",
    "\n",
    "      if file == \"\":\n",
    "        break\n",
    "\n",
    "      #print(\"Ga\")\n",
    "      \n",
    "      # Obteniendo el encoding por cada archivo\n",
    "      encoding, stringLog = getEncodingBumeran(file)\n",
    "\n",
    "      # Obteniendo los datos por cada archivo\n",
    "      candidate, stringLog = getCandidateBumeran(file, encoding, stringLog)\n",
    "      \n",
    "      self.results.append([candidate, stringLog])\n",
    "      self.queue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ac7ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateCandidateQueue(files, workersNumber):\n",
    "  q = queue.Queue()\n",
    "  for file in files:\n",
    "    q.put(file)\n",
    "  \n",
    "  for _ in range(workersNumber):\n",
    "    q.put(\"\")\n",
    "  \n",
    "  workers = []\n",
    "  for _ in range(workersNumber):\n",
    "      worker = Worker(q)\n",
    "      worker.start()\n",
    "      workers.append(worker)\n",
    "  \n",
    "  for worker in workers:\n",
    "      worker.join()\n",
    "  \n",
    "  result = []\n",
    "  for worker in workers:\n",
    "    result.extend(worker.results)\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1799b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateFiles(files, source):\n",
    "  # Definiendo el log de errores\n",
    "  log = \"\"\n",
    "\n",
    "  # Definiendo la data final de candidatos\n",
    "  data = []\n",
    "\n",
    "  results = iterateCandidateQueue(files, 1000)\n",
    "\n",
    "  print(len(results))\n",
    "  \n",
    "  \"\"\"# Iterando por cada archivo\n",
    "  for index, file in enumerate(files):\n",
    "    if (index % 250 == 0):\n",
    "      print(str(index) + \"/\" + str(len(files)) + \" archivos analizados\")\n",
    "\n",
    "    # Obteniendo el encoding por cada archivo\n",
    "    encoding, candidateLog = getEncodingBumeran(file)\n",
    "\n",
    "    # Obteniendo los datos por cada archivo\n",
    "    candidate, candidateLog = getCandidateBumeran(file, encoding, candidateLog)\n",
    "    \n",
    "    # Añadiendo los datos del candidato a la data final\n",
    "    if (candidate):\n",
    "      data.append(candidate)\n",
    "\n",
    "    # Añadiendo el dato al stringLog\n",
    "    if (candidateLog):\n",
    "      log = log + candidateLog + \"\\n\"\n",
    "  \"\"\"\n",
    "\n",
    "  data = [elem[0] for elem in results]\n",
    "  log = \"\\n\".join([elem[1] for elem in results])\n",
    "\n",
    "  # Realizando un ordenamiento por fecha de postulación, en orden descendente\n",
    "  sortedData = sorted(data, key=lambda x: (x[\"candidatePostulationDate\"]), reverse=True)\n",
    "\n",
    "  return sortedData, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "472c3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAndWriteMain(source):\n",
    "  # Definiendo la carpeta raiz de los archivos a buscar, según el origen\n",
    "  rootPath = bumeranRootPath\n",
    "\n",
    "  # Obteniendo la lista de archivos, según el origen\n",
    "  files = getFiles(rootPath, source)\n",
    "\n",
    "  # Iterando sobre los archivos, calculando la data y el log de error\n",
    "  data, stringLog = iterateFiles(files, source)\n",
    "\n",
    "  # Escribiendo la data de los candidatos (json y csv)\n",
    "  writeJson(data, os.path.join(intermFilesFolder, source + '.json'))\n",
    "  writeCsv(data, os.path.join(intermFilesFolder, source + '.csv'))\n",
    "\n",
    "  # Escribiendo el log de errores\n",
    "  writeTxt(stringLog, os.path.join(logsFolder, source, datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\") + \".txt\"), 'utf-16')\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8528b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAndWriteMergedMain(mainData):\n",
    "  mergedMainData = []\n",
    "  for elem in mainData:\n",
    "    if (elem):\n",
    "      mergedMainData.extend(elem)\n",
    "\n",
    "  writeJson(mergedMainData, os.path.join(mergedMainFolder, \"result.json\"), 'utf-8')\n",
    "  writeCsv(mergedMainData, os.path.join(mergedMainFolder, \"result.csv\"), 'utf-8')\n",
    "\n",
    "  return mergedMainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2925ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeData(mergedMainData):\n",
    "  df = pd.DataFrame(mergedMainData)\n",
    "\n",
    "  print(df.count())\n",
    "\n",
    "  columns = [elem for elem in df.columns if elem not in [\"candidatePostulationDate\", \"candidateFullName\"]]\n",
    "\n",
    "  for column in columns:\n",
    "    top10 = df[column].value_counts()[:dataVisualizationTopLimit]\n",
    "    print(top10)\n",
    "    y_axis = list(reversed(top10.index))\n",
    "    x_axis = list(reversed(top10.values))\n",
    "    plt.ylabel(column)\n",
    "    plt.barh(y_axis, x_axis)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1174eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9021c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio: 2023-05-16 11:17:00.674851\n",
      "Se inició el procesamiento\n",
      "Archivos originales: 10230\n",
      "Archivos filtrados solo con postulaciones: 10228\n",
      "Archivos filtrados solo con data completa: 10225\n",
      "10225\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m\n\u001b[0;32m     29\u001b[0m   \u001b[39m#print(counterIn)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m   \u001b[39m#print(counterOut)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 33\u001b[0m   main()\n",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m isMergedMain \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m#False\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[39m# Leyendo o calculando bumeran\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m bumeranData \u001b[39m=\u001b[39m readJson(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(intermFilesFolder, \u001b[39m'\u001b[39m\u001b[39mbumeran.json\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39mif\u001b[39;00m isLoadedBumeran \u001b[39melse\u001b[39;00m readAndWriteMain(\u001b[39m'\u001b[39;49m\u001b[39mbumeran\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSe terminó de procesar Bumeran\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[39m# Uniendo la data principal (bumeran + linkedin (ya no))\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36mreadAndWriteMain\u001b[1;34m(source)\u001b[0m\n\u001b[0;32m     13\u001b[0m writeCsv(data, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(intermFilesFolder, source \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     15\u001b[0m \u001b[39m# Escribiendo el log de errores\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m writeTxt(stringLog, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(logsFolder, source, datetime\u001b[39m.\u001b[39;49mdatetime\u001b[39m.\u001b[39;49mnow()\u001b[39m.\u001b[39;49mstrftime(\u001b[39m\"\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mY-\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mm-\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mH-\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mM-\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mS\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m), \u001b[39m'\u001b[39;49m\u001b[39mutf-16\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "Cell \u001b[1;32mIn[3], line 68\u001b[0m, in \u001b[0;36mwriteTxt\u001b[1;34m(data, pathTxt, encoding)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwriteTxt\u001b[39m(data, pathTxt, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     67\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(pathTxt, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39mencoding) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> 68\u001b[0m     f\u001b[39m.\u001b[39;49mwrite(data)\n",
      "\u001b[1;31mTypeError\u001b[0m: write() argument must be str, not list"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "  # Definiendo el inicio del proceso\n",
    "  startTime = datetime.datetime.now()\n",
    "  print(\"Inicio: \" + str(startTime))\n",
    "  print(\"Se inició el procesamiento\")\n",
    "\n",
    "  # El dataset principal que sea el de bumeran\n",
    "  # Los otros archivos que solo se usen para actualizar la variable objetivo\n",
    "  # Esto porque los otros orígenes están incompletos y costaría llenar los campos (incluso el excel que se hizo)\n",
    "  isLoadedBumeran = False\n",
    "  isMergedMain = False\n",
    "  #False\n",
    "\n",
    "  # Leyendo o calculando bumeran\n",
    "  bumeranData = readJson(os.path.join(intermFilesFolder, 'bumeran.json')) if isLoadedBumeran else readAndWriteMain('bumeran')\n",
    "  print(\"Se terminó de procesar Bumeran\")\n",
    "\n",
    "  # Uniendo la data principal (bumeran + linkedin (ya no))\n",
    "  mergedMainData = readJson(os.path.join(mergedMainFolder, 'result.json')) if isMergedMain else readAndWriteMergedMain([bumeranData])\n",
    "  print(\"Se terminó de unir la data principal\")\n",
    "\n",
    "  visualizeData(mergedMainData)\n",
    "  \n",
    "  # Definiendo el fin del proceso\n",
    "  endTime = datetime.datetime.now()\n",
    "  print(\"Fin: \" + str(endTime))\n",
    "  print(\"Tiempo: \" + str(endTime-startTime))\n",
    "\n",
    "  #print(counterIn)\n",
    "  #print(counterOut)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
