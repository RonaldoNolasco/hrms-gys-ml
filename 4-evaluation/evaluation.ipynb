{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "dad0ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate, cross_val_predict\n",
    "from sklearn.metrics import f1_score, precision_score, confusion_matrix, accuracy_score, recall_score\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "763cd073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "inputFolder = \"1-input\"\n",
    "processFolder = \"2-process\"\n",
    "outputFolder = \"3-output\"\n",
    "logsFolder = \"4-logs\"\n",
    "\n",
    "inputMainFolder = inputFolder + r\"\\main\"\n",
    "inputAlgorithmsFolder = inputFolder + r\"\\algorithms\"\n",
    "\n",
    "dataVisualizationTopLimit = 20\n",
    "\n",
    "testSize = 0.25\n",
    "randomState = 0\n",
    "partitionsNumber = 5\n",
    "samplingStrategy = 0.2\n",
    "percentileNumberStd = 90\n",
    "percentileNumberCorrelation = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "c14e4312",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "  \"K vecinos más cercanos\": KNeighborsClassifier(),\n",
    "  \"Máquina de vectores de soporte\": SVC(random_state=randomState),\n",
    "  \"Regresión logística\": LogisticRegression(random_state=randomState, max_iter=200),\n",
    "  \"Naive bayes gaussiano\": GaussianNB(),\n",
    "  \"Aumento de gradiente\": GradientBoostingClassifier(random_state=randomState),\n",
    "  \"Árbol de decisión\": DecisionTreeClassifier(random_state=randomState),\n",
    "  \"Bosque aleatorio\": RandomForestClassifier(random_state=randomState),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "919ac9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilitarias\n",
    "def readJson(path, encoding='utf-8', errors=None):\n",
    "  with open (path, \"r\", encoding=encoding, errors=errors) as f:\n",
    "    data = json.loads(f.read())\n",
    "  return data\n",
    "\n",
    "def writeJson(data, pathJson, encoding='utf-8'):\n",
    "  with open(pathJson, 'w', encoding=encoding) as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def readCsvAsDf(filePath, delimiter, encoding, header):\n",
    "  data = pd.read_csv(filePath, delimiter=delimiter,encoding=encoding, header=header)\n",
    "  return data\n",
    "\n",
    "def writeCsv(data, pathCsv, encoding='utf-8'):\n",
    "  with open(pathCsv, 'w', newline='', encoding=encoding) as f:\n",
    "    if data:\n",
    "      writer = csv.DictWriter(f, fieldnames=data[0].keys(), lineterminator='\\n')\n",
    "      writer.writeheader()\n",
    "      writer.writerows(data)\n",
    "    else:\n",
    "      f.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "311c6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingData(inputPath):\n",
    "  # Leyendo datos\n",
    "  df = readCsvAsDf(inputPath, ',', 'utf-8', 0)\n",
    "\n",
    "  # Balanceo de datos: Sobremuestreo aleatorio (oversampling) (1 de cada 5)\n",
    "  objectiveColumn = \"contratado\"\n",
    "  dictResults = dict(df[objectiveColumn].value_counts().sort_index())\n",
    "\n",
    "  maxKey = max(dictResults, key=dictResults.get)\n",
    "  maxValue = max(dictResults.values())\n",
    "\n",
    "  dfClassMaxKey = df[df[objectiveColumn] == maxKey]\n",
    "\n",
    "  for key, value in dictResults.items():\n",
    "    if key != maxKey:\n",
    "      dfClass = df[df[objectiveColumn] == key]\n",
    "      dfClassSampled = dfClass.sample(int(maxValue * samplingStrategy), random_state=randomState, replace=True)\n",
    "      dfClassMaxKey = pd.concat([dfClassMaxKey, dfClassSampled],axis=0)\n",
    "\n",
    "  df = dfClassMaxKey\n",
    "\n",
    "  # Aleatorizacion del orden de los registros para evitar sesgos(filas)\n",
    "  df = df.sample(frac = 1, random_state=randomState).reset_index(drop=True)\n",
    "\n",
    "  # Aplicando OrdinalEncoding a las variables categóricas ordinales()\n",
    "  encoder = OrdinalEncoder(categories=[[ \"Abandonado\", \"En Curso\", \"Graduado\" ]])\n",
    "  encoder.fit(df[[\"estadoUltimoEstudio\"]])\n",
    "  df[\"estadoUltimoEstudio\"] = encoder.transform(df[[\"estadoUltimoEstudio\"]])\n",
    "  encoder = OrdinalEncoder(categories=[[ \"Otro\", \"Secundario\", \"Terciario/Tecnico\", \"Universitario\", \"Posgrado\", \"Master\", \"Doctorado\" ]])\n",
    "  encoder.fit(df[[\"gradoUltimoEstudio\"]])\n",
    "  df[\"gradoUltimoEstudio\"] = encoder.transform(df[[\"gradoUltimoEstudio\"]])\n",
    "\n",
    "  # Aplicando OneHotEncoding a las variables categóricas cardinales (transformación a numéricas mediante columnas)\n",
    "  categoricalColumns = [columnName for columnName, columnType in df.dtypes.to_dict().items() if columnName not in [ \"contratado\" ] and columnType == \"object\" ]\n",
    "  categoricalCardinalColumns = [columnName for columnName in categoricalColumns if columnName not in [ \"estadoUltimoEstudio\", \"gradoUltimoEstudio\" ]]\n",
    "  for column in categoricalCardinalColumns:\n",
    "    dummies = pd.get_dummies(df[[column]], prefix=column, dummy_na=True)\n",
    "    df = pd.concat([df, dummies], axis = 1)\n",
    "    df = df.drop(columns=[column])\n",
    "\n",
    "  # Aplicando MinMaxScaler a las variables numéricas (normalización) (esto tambien incluye a lastEducationStatus y lastEducationDegree, ya numéricas)\n",
    "  # Algunas quedaran en 0.9999, esto porque no todas manejan la misma escala (sin decimales, o solo un decimal)\n",
    "  numericalColumns = [columnName for columnName, columnType in df.dtypes.to_dict().items() if columnName not in [ \"contratado\" ] and columnType == \"float64\" ]\n",
    "  for column in numericalColumns:\n",
    "    df[column] = df[column].fillna(0.0)\n",
    "  mms = MinMaxScaler()\n",
    "  df[numericalColumns] = mms.fit_transform(df[numericalColumns])\n",
    "\n",
    "  # Eliminando columnas con varianza cercana a cero, dejando el 10% de columnas con mayor varianza (variables no afectan en el resultado del modelo)\n",
    "  df.loc['std'] = df.std()\n",
    "  stdArray = df.iloc[len(df)-1]\n",
    "  nthPercentileStd = np.percentile(stdArray, percentileNumberStd)\n",
    "  df = df.transpose()\n",
    "  df = df[df[\"std\"]>nthPercentileStd]\n",
    "  df = df.transpose()\n",
    "  df = df.drop(['std'], axis=0)\n",
    "\n",
    "  # Eliminando columnas con correlación cercana a uno, dejando el 90% de columnas con menor correlación\n",
    "  correlationMatrix = df.corr().abs()\n",
    "  correlationMatrix[correlationMatrix == 1.0] = 0.0\n",
    "  maxCorrelationValues = [max(correlationMatrix[column]) for column in correlationMatrix.columns]\n",
    "  nthPercentileCorrelation = np.percentile(maxCorrelationValues, percentileNumberCorrelation)\n",
    "  highCorrelationColumns = [column for column in correlationMatrix.columns if max(correlationMatrix[column]) > nthPercentileCorrelation]\n",
    "  df = df.drop(highCorrelationColumns, axis=1)\n",
    "\n",
    "  # Regresando el dataframe a array de dicts\n",
    "  preprocessedData = df.to_dict('records')\n",
    "\n",
    "  #writeJson(preprocessedData, os.path.join(processFolder, 'result.json'), 'utf-8') #Pesa mucho y ni se usa\n",
    "  writeCsv(preprocessedData, os.path.join(processFolder, 'result.csv'), 'utf-8')\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "49ddd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X, y):\n",
    "  # Dividiendo los dataframes de entrenamiento y prueba\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = testSize, random_state=randomState)\n",
    "\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "c2be135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(X_train, X_test, y_train, y_test, algorithm):\n",
    "  # Creación del clasificador\n",
    "  clf = classifiers[algorithm]\n",
    "\n",
    "  # Entrenamiento del clasificador\n",
    "  clf.fit(X_train, y_train)\n",
    "\n",
    "  # Calculando la predicción del modelo con la data de prueba\n",
    "  y_pred = clf.predict(X_test)\n",
    "\n",
    "  return y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "e1887988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(y_train, y_test, y_pred, startDate, endDate, algorithm, partitionNumber):\n",
    "  trainRows = len(y_train)\n",
    "  testRows = len(y_test)\n",
    "\n",
    "  # Calculando la exactitud del modelo\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "  # Calculando la precisión del modelo\n",
    "  precision = precision_score(y_test, y_pred)\n",
    "\n",
    "  # Calculando la sensibilidad del modelo\n",
    "  recall = recall_score(y_test, y_pred)\n",
    "\n",
    "  # Calculando el valor F del modelo (robustez)\n",
    "  f1Score = f1_score(y_test, y_pred)\n",
    "\n",
    "  # Calculando el promedio de métricas\n",
    "  metricsList = [accuracy, precision, recall, f1Score]\n",
    "  metricsMean = sum(metricsList) / len(metricsList)\n",
    "\n",
    "  # Calculando el tiempo de ejecución del modelo\n",
    "  executionTime = (endDate - startDate).total_seconds()\n",
    "  formatExecutionTime = \"{:.2f}\".format(executionTime) + \"s\"\n",
    "  formatAverageTime = \"{:.2f}\".format(executionTime*1000/(trainRows + testRows)) + \"ms\"\n",
    "\n",
    "  # Obteniendo la matriz de confusión\n",
    "  confussionMatrix = str(confusion_matrix(y_test, y_pred).tolist())\n",
    "  \n",
    "  return {\n",
    "    \"algoritmo\": algorithm,\n",
    "    \"particion\": \"Partición {}\".format(partitionNumber),\n",
    "    \"registrosEntrenamiento\": trainRows,\n",
    "    \"registrosPrueba\": testRows,\n",
    "    \"proporcionSobremuestreo\": samplingStrategy,\n",
    "    \"tiempoEjecucion\": formatExecutionTime,\n",
    "    \"matrizConfusion\": confussionMatrix,\n",
    "    \"exactitud\": \"{:.2%}\".format(accuracy),\n",
    "    \"precision\": \"{:.2%}\".format(precision),\n",
    "    \"sensibilidad\": \"{:.2%}\".format(recall),\n",
    "    \"robustez\": \"{:.2%}\".format(f1Score),\n",
    "    \"promedioMetricas\": \"{:.2%}\".format(metricsMean),\n",
    "    \"tiempoPromedio\": formatAverageTime\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "ccf7a9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(df, algorithm):\n",
    "  # Creando el arreglo de metricas de cada algoritmo\n",
    "  partitionsMetricsList = []\n",
    "\n",
    "  # Lectura de las variables de características y objetivo\n",
    "  objectiveColumn = \"contratado\"\n",
    "  X = df.drop([objectiveColumn], axis=1)\n",
    "  y = df[objectiveColumn]\n",
    "\n",
    "  # Definiendo el numero de folds\n",
    "  kFolds = KFold(partitionsNumber, shuffle=True, random_state=randomState)\n",
    "\n",
    "  # Iterando por cada fold\n",
    "  for index, (train_index, test_index) in enumerate(kFolds.split(X, y)):\n",
    "    X_train = X.iloc[train_index, :]\n",
    "    X_test = X.iloc[test_index, :]\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "\n",
    "    # Mostrando que algoritmo se usa\n",
    "    print(\"Ejecutando para la partición: {}\".format(str(index+1)))\n",
    "\n",
    "    # Inicio de ejecución\n",
    "    startDate = datetime.datetime.now()\n",
    "\n",
    "    # Realizar entrenamiento del modelo\n",
    "    y_test, y_pred = trainModel(X_train, X_test, y_train, y_test, algorithm)\n",
    "\n",
    "    # Fin de ejecución del modelo\n",
    "    endDate = datetime.datetime.now()\n",
    "\n",
    "    # Obteniendo las métricas de la partición del modelo\n",
    "    partitionMetrics = getMetrics(y_train, y_test, y_pred, startDate, endDate, algorithm, index+1)\n",
    "\n",
    "    # Añadiendo la métrica de la partición a la lista de métricas\n",
    "    partitionsMetricsList.append(partitionMetrics)\n",
    "\n",
    "  writeJson(partitionsMetricsList, os.path.join(outputFolder, 'result.json'), 'utf-8')\n",
    "  writeCsv(partitionsMetricsList, os.path.join(outputFolder, 'result.csv'), 'utf-8')\n",
    "\n",
    "  return partitionsMetricsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "83f3c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  # Definiendo el inicio del proceso\n",
    "  startTime = datetime.datetime.now()\n",
    "  print(\"Inicio: \" + str(startTime))\n",
    "\n",
    "  isPreprocessed = True\n",
    "  isEvaluated = False\n",
    "\n",
    "  # Preprocesando los datos\n",
    "  print(\"Preprocesando datos\")\n",
    "  df = readCsvAsDf(os.path.join(processFolder, \"result.csv\"), \",\", \"utf-8\", 0) if isPreprocessed else preprocessingData(os.path.join(inputMainFolder, \"result.csv\"))\n",
    "\n",
    "  # Obteniendo la lista de métricas del modelado\n",
    "  print(\"Obteniendo lista de métricas del modelado\")\n",
    "  algorithmsMetricsList = readJson(os.path.join(inputAlgorithmsFolder, 'result.json'))\n",
    "\n",
    "  # Elegir el algoritmo con mayor promedio de métricas\n",
    "  maxAverageAlgorithm = max(algorithmsMetricsList, key=lambda x:x[\"promedioMetricas\"])\n",
    "\n",
    "  print(\"Algoritmo con mayor promedio de métricas: {}\".format(maxAverageAlgorithm[\"algoritmo\"]))\n",
    "\n",
    "  # Evaluando el modelo\n",
    "  print(\"Evaluando el modelo\")\n",
    "  partitionsMetricsList = readJson(os.path.join(outputFolder, 'result.json')) if isEvaluated else evaluation(df, maxAverageAlgorithm[\"algoritmo\"])\n",
    "\n",
    "  # Elegir la partición con mayor promedio de métricas\n",
    "  maxAveragePartition = max(partitionsMetricsList, key=lambda x:x[\"promedioMetricas\"])\n",
    "\n",
    "  print(\"Partición con mayor promedio de métricas: {}\".format(maxAveragePartition[\"particion\"]))\n",
    "  print(\"Promedio de métricas: {}\".format(maxAveragePartition[\"promedioMetricas\"]))\n",
    "  print(\"Exactitud: {}\".format(maxAveragePartition[\"exactitud\"]))\n",
    "  print(\"Precisión: {}\".format(maxAveragePartition[\"precision\"]))\n",
    "  print(\"Sensibilidad: {}\".format(maxAveragePartition[\"sensibilidad\"]))\n",
    "  print(\"Robustez: {}\".format(maxAveragePartition[\"robustez\"]))\n",
    "  print(\"Tiempo promedio: {}\".format(maxAveragePartition[\"tiempoPromedio\"]))\n",
    "\n",
    "  # Definiendo el fin del proceso\n",
    "  endTime = datetime.datetime.now()\n",
    "  print(\"Fin: \" + str(endTime))\n",
    "  print(\"Tiempo: \" + str(endTime-startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "7c95c684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio: 2023-06-07 13:13:24.936611\n",
      "Preprocesando datos\n",
      "Obteniendo lista de métricas del modelado\n",
      "Algoritmo con mayor promedio de métricas: Bosque aleatorio\n",
      "Evaluando el modelo\n",
      "Ejecutando para la partición: 1\n",
      "Ejecutando para la partición: 2\n",
      "Ejecutando para la partición: 3\n",
      "Ejecutando para la partición: 4\n",
      "Ejecutando para la partición: 5\n",
      "Partición con mayor promedio de métricas: Partición 4\n",
      "Promedio de métricas: 99.13%\n",
      "Exactitud: 99.67%\n",
      "Precisión: 100.00%\n",
      "Sensibilidad: 97.92%\n",
      "Robustez: 98.95%\n",
      "Tiempo promedio: 0.31ms\n",
      "Fin: 2023-06-07 13:13:45.118670\n",
      "Tiempo: 0:00:20.182059\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
