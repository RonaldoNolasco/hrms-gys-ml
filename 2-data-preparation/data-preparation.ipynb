{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0a593920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "import datetime\n",
    "import traceback\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import jellyfish\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1a8b9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "inputFolder = \"1-input\"\n",
    "processFolder = \"2-process\"\n",
    "outputFolder = \"3-output\"\n",
    "otherOutputFolder = \"../3-modelling/1-input/\"\n",
    "logsFolder = \"4-logs\"\n",
    "\n",
    "inputMainFolder = inputFolder + r\"\\main\"\n",
    "inputMastersFolder = inputFolder + r\"\\masters\"\n",
    "\n",
    "processMainFolder = processFolder + r\"\\main\"\n",
    "processSupportFolder = processFolder + r\"\\support\"\n",
    "\n",
    "dataVisualizationTopLimit = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "22025f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilitarias\n",
    "def find_nth(haystack, needle, n):\n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def find_nth_right(haystack, needle, n):\n",
    "    start = haystack.rfind(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.rfind(needle, 0, start-len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def parseLineBreaksAndAccents(text):\n",
    "  return unidecode(\" \".join(text.split()))\n",
    "\n",
    "def parseNames(text):\n",
    "  return text.strip().title()\n",
    "\n",
    "def findTags(tag, color):\n",
    "  return tag.find(\"span\", {\"style\": 'font-size:10.0pt;font-family:\"Arial\",sans-serif;mso-fareast-font-family:\\n\"Times New Roman\";color:' + color })\n",
    "\n",
    "def getChildIndex(mainChildTags, title, color):\n",
    "  return next((index for index, tag in enumerate(mainChildTags) if ( parseNames(parseLineBreaksAndAccents(findTags(tag, color).text)) == title if findTags(tag, color) else False )), None)\n",
    "\n",
    "def getSectionsIndexes(mainChildTags, color):\n",
    "  sectionsIndexes = []\n",
    "  sectionsTitle = [\"Objetivo Laboral\", \"Experiencia Laboral\", \"Educacion\", \"Informatica\", \"Idiomas\", \"Otros Conocimientos\"]\n",
    "  \n",
    "  for sectionTitle in sectionsTitle:\n",
    "    sectionIndex = getChildIndex(mainChildTags, sectionTitle, color)\n",
    "    sectionsIndexes.append(sectionIndex)\n",
    "  \n",
    "  sectionsIndexes.append(len(mainChildTags)-1)\n",
    "  return sectionsIndexes\n",
    "\n",
    "def getNextSectionIndexValid(sectionsIndexes, i):\n",
    "  while(not sectionsIndexes[i]):\n",
    "    i = i + 1\n",
    "\n",
    "  return sectionsIndexes[i]\n",
    "\n",
    "def getStartAndEndIndex(sectionsIndexes, i):\n",
    "  return sectionsIndexes[i], getNextSectionIndexValid(sectionsIndexes, i+1)\n",
    "\n",
    "def readJson(path, encoding='utf-8', errors=None):\n",
    "  with open (path, \"r\", encoding=encoding, errors=errors) as f:\n",
    "    data = json.loads(f.read())\n",
    "  return data\n",
    "\n",
    "def writeJson(data, pathJson, encoding='utf-8'):\n",
    "  with open(pathJson, 'w', encoding=encoding) as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def writeCsv(data, pathCsv, encoding='utf-8'):\n",
    "  with open(pathCsv, 'w', newline='', encoding=encoding) as f:\n",
    "    if data:\n",
    "      writer = csv.DictWriter(f, fieldnames=data[0].keys(), lineterminator='\\n')\n",
    "      writer.writeheader()\n",
    "      writer.writerows(data)\n",
    "    else:\n",
    "      f.write(\"\")\n",
    "\n",
    "def writeTxt(data, pathTxt, encoding='utf-8'):\n",
    "  with open(pathTxt, 'w', encoding=encoding) as f:\n",
    "    f.write(data)\n",
    "\n",
    "def readAndWriteSupport(folder, file, writeFileName, readEncoding = 'utf-8', writeEncoding = 'utf-8'):\n",
    "  with open(os.path.join(folder, file), \"r\", encoding=readEncoding) as file:\n",
    "    data = [{k: parseNames(v) for k, v in row.items()} for row in csv.DictReader(file, skipinitialspace=True)]\n",
    "\n",
    "  with open(os.path.join(processSupportFolder, writeFileName), 'w', encoding=writeEncoding) as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "  \n",
    "  return data\n",
    "\n",
    "def readAndWriteInHome(folder, writeFileName):\n",
    "  folderFiles = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "  attendees = []\n",
    "  \n",
    "  for file in folderFiles:\n",
    "    # Detectar si tienen el To: en el texto\n",
    "    try:\n",
    "      # Detectar el encoding\n",
    "      encoding = \"utf-16\"\n",
    "      with open(file, \"r\") as f:\n",
    "        encoding = 'windows-1252' if \"charset=windows-1252\" in f.read() else 'utf-16'\n",
    "\n",
    "      # El .read() manda el cursor al final del archivo, por eso si se usa 2 veces, la 2da vez no encuentra nada\n",
    "      with open(file, \"r\", encoding=encoding) as f:\n",
    "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "        #print(str(soup))\n",
    "        if '<span style=\"color:black\">To:<span style=\"mso-tab-count:1\">' in parseLineBreaksAndAccents(str(soup)):\n",
    "          emailAttendeesText = parseLineBreaksAndAccents(soup.find_all(\"span\", {\"style\": 'color:black'})[5].text)\n",
    "          emailAttendeesList = emailAttendeesText.split(\"; \")\n",
    "          for emailAttendee in emailAttendeesList:\n",
    "            if emailAttendee not in attendees:\n",
    "              attendees.append(emailAttendee)\n",
    "    except Exception as e:\n",
    "      print(file)\n",
    "      traceback.print_exc()\n",
    "      print()\n",
    "      pass\n",
    "\n",
    "  data = [ {\"name\": parseNames(participant)} for participant in attendees ]\n",
    "\n",
    "  with open(os.path.join(processSupportFolder, writeFileName), 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "  return data\n",
    "\n",
    "def getCompare(data, fields):\n",
    "  return (list(set([\" \".join(x for x in [elem[y] for y in fields] if x) for elem in data if elem])))\n",
    "\n",
    "def readSupport(file, readEncoding = 'utf-8'):\n",
    "  with open(os.path.join(file), \"r\", encoding=readEncoding) as file:\n",
    "    data = [{k: parseNames(v) for k, v in row.items()} for row in csv.DictReader(file, skipinitialspace=True)]\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "88d18df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAndWritePreprocessedData(preprocessedData, masters):\n",
    "  ## Decisiones propias para el modelo\n",
    "  # Transformar la data a dataframe\n",
    "  df = pd.DataFrame(preprocessedData)\n",
    "\n",
    "  for i in range(0, len(masters)):\n",
    "    masters[i] = pd.DataFrame(masters[i])\n",
    "\n",
    "  # Eliminar los campos que ya no se usarán\n",
    "  columnsToDelete = [\n",
    "    \"candidatePostulationDate\", \"jobId\", \"candidateFullName\", \"candidateDocumentNumber\", \"candidateBirthDate\", \"candidateAddress\", \"candidateHomeNumber\", \"candidateCellphoneNumber\", \"candidateEmail\", \"workObjetive\", \"lastWorkDays\", \"lastWorkDescription\", \"lastEducationDays\", \"yearsOfExperience\", \"yearsOfStudy\"\n",
    "  ]\n",
    "  # El tema con las ultimas 2 es que puede ser 0 cuando hay o no data de experiencia y educacion, reemplazar el NaN desde el comienzo en el dataframe (understanding)\n",
    "  df = df.drop(columns=columnsToDelete)\n",
    "\n",
    "  # Aplicar las equivalencias a los campos: jobProfileName, lastWorkCompany, lastWorkArea, lastWorkName, lastEducationCompany, lastEducationArea, lastEducationName\n",
    "  equivalences = [\"jobProfileName\", \"lastWorkCompany\", \"lastWorkArea\", \"lastWorkName\", \"lastEducationCompany\", \"lastEducationArea\", \"lastEducationName\"]\n",
    "\n",
    "  # Mejorar las equivalencias poco a poco\n",
    "  for index, equivalence in enumerate(equivalences):\n",
    "    df = pd.merge(df, masters[index], on=equivalence)\n",
    "    df[equivalence] = df[\"equivalence\"]\n",
    "    df = df.drop(columns=[\"equivalence\", \"count\", \"equals\"])\n",
    "    \n",
    "  print(len(df))\n",
    "\n",
    "  ## Decisiones por la metodología\n",
    "\n",
    "  # Analisis de valores nulos\n",
    "  # Reemplazando las cadenas vacias a NaN\n",
    "  categoricalColumns = [columnName for columnName, columnType in df.dtypes.to_dict().items() if columnName not in [ \"hired\" ] and columnType == \"object\" ]\n",
    "  for column in categoricalColumns:\n",
    "    df[column] = df[column].replace('',np.nan,regex = True)\n",
    "\n",
    "  # Reemplazando los ceros a NaN\n",
    "  numericalColumns = [columnName for columnName, columnType in df.dtypes.to_dict().items() if columnName not in [ \"hired\" ] and columnType == \"int64\" ]\n",
    "  for column in numericalColumns:\n",
    "    df[column] = df[column].replace(0,np.nan)\n",
    "\n",
    "  # Obteniendo el porcentaje de nulos por columna\n",
    "  percentMissing = df.isnull().sum() * 100 / len(df)\n",
    "  missingValueDf = pd.DataFrame({'columnName': df.columns, 'percentMissing': percentMissing })\n",
    "  missingValueDf = missingValueDf.sort_values('percentMissing', ascending=False)\n",
    "  print(missingValueDf)\n",
    "  \n",
    "  # Al ninguno superar el 30% de nulos, no se eliminará ninguna columna\n",
    "\n",
    "  # Regresando el dataframe a array de dicts\n",
    "  preprocessedData = df.to_dict('records')\n",
    "\n",
    "  writeJson(preprocessedData, os.path.join(outputFolder, 'result.json'), 'utf-8')\n",
    "  writeCsv(preprocessedData, os.path.join(outputFolder, 'result.csv'), 'utf-8')\n",
    "\n",
    "  writeJson(preprocessedData, os.path.join(otherOutputFolder, 'result.json'), 'utf-8')\n",
    "  writeCsv(preprocessedData, os.path.join(otherOutputFolder, 'result.csv'), 'utf-8')\n",
    "\n",
    "  return preprocessedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1994b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeData(preprocessedData):\n",
    "  df = pd.DataFrame(preprocessedData)\n",
    "\n",
    "  print(df.count())\n",
    "\n",
    "  columns = [elem for elem in df.columns]\n",
    "\n",
    "  for column in columns:\n",
    "    topDf = df[column].value_counts()[:dataVisualizationTopLimit]\n",
    "    print(topDf)\n",
    "    y_axis = list(reversed(topDf.index))\n",
    "    x_axis = list(reversed(topDf.values))\n",
    "    plt.ylabel(column)\n",
    "    plt.barh(y_axis, x_axis)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9125a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  # Definiendo el inicio del proceso\n",
    "  startTime = datetime.datetime.now()\n",
    "  print(\"Inicio: \" + str(startTime))\n",
    "  print(\"Se inició el procesamiento\")\n",
    "\n",
    "  # Leyendo la data obtenida en el entendimiento de los datos\n",
    "  bumeranData = readJson(os.path.join(inputMainFolder, 'result.json'))\n",
    "  # Leyendo archivos maestros\n",
    "  jobProfileName = readSupport(os.path.join(inputMastersFolder, 'jobProfileName.csv'))\n",
    "  lastWorkCompany = readSupport(os.path.join(inputMastersFolder, 'lastWorkCompany.csv'))\n",
    "  lastWorkArea = readSupport(os.path.join(inputMastersFolder, 'lastWorkArea.csv'))\n",
    "  lastWorkName = readSupport(os.path.join(inputMastersFolder, 'lastWorkName.csv'))\n",
    "  lastEducationCompany = readSupport(os.path.join(inputMastersFolder, 'lastEducationCompany.csv'))\n",
    "  lastEducationArea = readSupport(os.path.join(inputMastersFolder, 'lastEducationArea.csv'))\n",
    "  lastEducationName = readSupport(os.path.join(inputMastersFolder, 'lastEducationName.csv'))\n",
    "\n",
    "  isPreprocessed = False\n",
    "\n",
    "  # Aplicando los datos de los maestros y validaciones\n",
    "  preprocessedData = readJson(os.path.join(outputFolder, 'result.json')) if isPreprocessed else readAndWritePreprocessedData(bumeranData, [jobProfileName, lastWorkCompany, lastWorkArea, lastWorkName, lastEducationCompany, lastEducationArea, lastEducationName])\n",
    "  print(\"Se terminó el preprocesamiento\")\n",
    "\n",
    "  #visualizeData(preprocessedData)\n",
    "\n",
    "  # Definiendo el fin del proceso\n",
    "  endTime = datetime.datetime.now()\n",
    "  print(\"Fin: \" + str(endTime))\n",
    "  print(\"Tiempo: \" + str(endTime-startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0eaed034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio: 2023-05-22 01:13:20.644507\n",
      "Se inició el procesamiento\n",
      "10225\n",
      "                                          columnName  percentMissing\n",
      "otherSkills                              otherSkills       23.198044\n",
      "technicalSkills                      technicalSkills       11.931540\n",
      "languages                                  languages        7.550122\n",
      "candidateCivilStatus            candidateCivilStatus        6.092910\n",
      "lastWorkCompany                      lastWorkCompany        5.574572\n",
      "lastWorkArea                            lastWorkArea        5.496333\n",
      "lastWorkName                            lastWorkName        5.496333\n",
      "worksNumber                              worksNumber        5.496333\n",
      "lastWorkCountry                      lastWorkCountry        5.496333\n",
      "salary                                        salary        4.146699\n",
      "lastEducationArea                  lastEducationArea        1.995110\n",
      "lastEducationCompany            lastEducationCompany        1.789731\n",
      "lastEducationName                  lastEducationName        1.388753\n",
      "lastEducationStatus              lastEducationStatus        1.212714\n",
      "lastEducationCountry            lastEducationCountry        1.202934\n",
      "lastEducationDegree              lastEducationDegree        1.202934\n",
      "studiesNumber                          studiesNumber        1.202934\n",
      "candidateResidenceCountry  candidateResidenceCountry        0.616137\n",
      "candidateBirthCountry          candidateBirthCountry        0.616137\n",
      "jobProfileName                        jobProfileName        0.000000\n",
      "hired                                          hired        0.000000\n",
      "Se terminó el preprocesamiento\n",
      "Fin: 2023-05-22 01:13:21.527771\n",
      "Tiempo: 0:00:00.883264\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
