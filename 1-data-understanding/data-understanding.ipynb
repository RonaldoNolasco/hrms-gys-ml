{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "6793a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import os\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from unidecode import unidecode\n",
    "import datetime\n",
    "import traceback\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import jellyfish\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from threading import Thread\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "125e31d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "sourceDataFolder = \"1-source-data\"\n",
    "\n",
    "bumeraniterationNumber = \"5\"\n",
    "bumeranRootPath = sourceDataFolder + r\"\\main\\bumeran\\iteration-\" + bumeraniterationNumber\n",
    "\n",
    "intermFilesFolder = \"2-intermediate-files\"\n",
    "mergedMainFolder = \"3-results\"\n",
    "logsFolder = \"4-logs\"\n",
    "\n",
    "dataVisualizationTopLimit = 20\n",
    "\n",
    "counterIn = 0\n",
    "counterOut = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "85e41e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilitarias\n",
    "def find_nth(haystack, needle, n):\n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def find_nth_right(haystack, needle, n):\n",
    "    start = haystack.rfind(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.rfind(needle, 0, start-len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def parseLineBreaksAndAccents(text):\n",
    "  return unidecode(\" \".join(text.split()))\n",
    "\n",
    "def parseNames(text):\n",
    "  return text.strip().title()\n",
    "\n",
    "def findTags(tag, color):\n",
    "  #if tag.find(\"span\", {\"style\": 'font-size:10.0pt;font-family:\"Arial\",sans-serif;mso-fareast-font-family:\\n\"Times New Roman\";color:' + color }) is not None: \n",
    "  #  print(parseNames(parseLineBreaksAndAccents(tag.find(\"span\", {\"style\": 'font-size:10.0pt;font-family:\"Arial\",sans-serif;mso-fareast-font-family:\\n\"Times New Roman\";color:' + color }).text)))\n",
    "  return tag.find(\"span\", {\"style\": 'font-size:10.0pt;font-family:\"Arial\",sans-serif;mso-fareast-font-family:\\n\"Times New Roman\";color:' + color })\n",
    "\n",
    "def getChildIndex(mainChildTags, title, color):\n",
    "  return next((index for index, tag in enumerate(mainChildTags) if ( parseNames(parseLineBreaksAndAccents(findTags(tag, color).text)) == title if findTags(tag, color) else False )), None)\n",
    "\n",
    "def getSectionsIndexes(mainChildTags, color):\n",
    "  sectionsIndexes = []\n",
    "  sectionsTitle = [\"Objetivo Laboral\", \"Experiencia Laboral\", \"Educacion\", \"Informatica\", \"Idiomas\", \"Otros Conocimientos\"]\n",
    "  \n",
    "  for sectionTitle in sectionsTitle:\n",
    "    sectionIndex = getChildIndex(mainChildTags, sectionTitle, color)\n",
    "    sectionsIndexes.append(sectionIndex)\n",
    "  \n",
    "  sectionsIndexes.append(len(mainChildTags)-1)\n",
    "  return sectionsIndexes\n",
    "\n",
    "def getNextSectionIndexValid(sectionsIndexes, i):\n",
    "  while(not sectionsIndexes[i]):\n",
    "    i = i + 1\n",
    "\n",
    "  return sectionsIndexes[i]\n",
    "\n",
    "def getStartAndEndIndex(sectionsIndexes, i):\n",
    "  return sectionsIndexes[i], getNextSectionIndexValid(sectionsIndexes, i+1)\n",
    "\n",
    "def readJson(path, encoding='utf-8', errors=None):\n",
    "  with open (path, \"r\", encoding=encoding, errors=errors) as f:\n",
    "    data = json.loads(f.read())\n",
    "  return data\n",
    "\n",
    "def writeJson(data, pathJson, encoding='utf-8'):\n",
    "  with open(pathJson, 'w', encoding=encoding) as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def writeCsv(data, pathCsv, encoding='utf-8'):\n",
    "  with open(pathCsv, 'w', newline='', encoding=encoding) as f:\n",
    "    if data:\n",
    "      writer = csv.DictWriter(f, fieldnames=data[0].keys(), lineterminator='\\n')\n",
    "      writer.writeheader()\n",
    "      writer.writerows(data)\n",
    "    else:\n",
    "      f.write(\"\")\n",
    "\n",
    "def writeTxt(data, pathTxt, encoding='utf-8'):\n",
    "  with open(pathTxt, 'w', encoding=encoding) as f:\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "bbd47f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones principales\n",
    "def getFiles(rootPath, source):\n",
    "  # Obteniendo todas las carpetas de los perfiles\n",
    "  folders = [f for f in os.listdir(rootPath) if os.path.isdir(os.path.join(rootPath, f))]\n",
    "\n",
    "  # Definiendo la lista de archivos final\n",
    "  files = []\n",
    "\n",
    "  # Iterando sobre las carpetas\n",
    "  for folder in folders:\n",
    "    # Obteniendo los archivos por cada carpeta\n",
    "    folderFiles = [os.path.join(rootPath, folder, f) for f in os.listdir(os.path.join(rootPath, folder)) if os.path.isfile(os.path.join(rootPath, folder, f))]\n",
    "\n",
    "    # Agregando esos archivos a la lista de archivos final\n",
    "    if folderFiles:\n",
    "      files.extend(folderFiles)\n",
    "\n",
    "  print(\"Archivos originales: \" + str(len(files)))\n",
    "\n",
    "  # Filtros específicos por cada origen\n",
    "  if source == \"bumeran\":\n",
    "    filesFiltered = [f for f in files if (\"\\Has recibido un CV para el aviso\" in f)]\n",
    "  else:\n",
    "    filesFiltered = files\n",
    "\n",
    "  print(\"Archivos filtrados solo con postulaciones: \" + str(len(filesFiltered)))\n",
    "\n",
    "  # Filtros para considerar un archivo especifico\n",
    "  filterIsIn = [\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\analista-calidad\\Has recibido un CV para el aviso _Analista de Calidad_(76).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\analista-funcional\\Has recibido un CV para el aviso _Analista Funcional_(110).html\"\n",
    "    #r\"1-source-data\\main\\linkedin\\iteration-2\\bandeja-entrada\\New application_ .NET Developer from Cesar Ospino.html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\a-considerar\\Has recibido un CV para el aviso _Practicante de Infrastructure & Cloud_(1).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\a-considerar\\Has recibido un CV para el aviso _Practicante de Infrastructure & Cloud_.html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\analista-calidad\\Has recibido un CV para el aviso _Analista de Calidad Sr. (Test Lead)_(1).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\analista-calidad\\Has recibido un CV para el aviso _Analista de Calidad_(1).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\bandeja-entrada\\Has recibido un CV para el aviso _Analista Programador Senior_Full Stack_(2).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\bandeja-entrada\\Has recibido un CV para el aviso _Analista Programador_(6).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\programador-net\\Has recibido un CV para el aviso _Programador .Net Senior_(4).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\bandeja-entrada\\Has recibido un CV para el aviso _Analista de Sistemas_(24).html\"\n",
    "    # 5 casos con estudios en blanco\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\bandeja-entrada\\Has recibido un CV para el aviso _Especialista en Analítica Jr_(50).html\",\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\bandeja-entrada\\Has recibido un CV para el aviso _Gestor de Servicios de TI_(132).html\",\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\bandeja-entrada-asistente-social\\Has recibido un CV para el aviso _Asistente Social_(173).html\",\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\bandeja-entrada-asistente-social\\Has recibido un CV para el aviso _Asistente Social_(71).html\",\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\ejecutivo-comercial\\Has recibido un CV para el aviso _Ejecutivo Comercial TI_(38).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-4\\bandeja-entrada-asistente-social\\Has recibido un CV para el aviso _Asistente Social_(196).html\"\n",
    "    #r\"1-source-data\\main\\bumeran\\iteration-5\\bandeja-entrada-2019-and-before\\Has recibido un CV para el aviso _Desarrollador.NET_(1).html\"\n",
    "    r\"1-source-data\\main\\bumeran\\iteration-5\\bandeja-entrada-2019-and-before\\Has recibido un CV para el aviso _Desarrollador. Net_(26).html\"\n",
    "  ]\n",
    "  filesFilteredIsIn = [f for f in filesFiltered if ( f in filterIsIn if filterIsIn else True )]\n",
    "\n",
    "  # Filtros para omitir un archivo especifico\n",
    "  filterIsNotIn = [\n",
    "    # 3 archivos con data incompleta (se mantiene en iteracion 4 y 5)\n",
    "    r\"1-source-data\\main\\bumeran\\iteration-5\\bandeja-entrada-2022\\Has recibido un CV para el aviso _Analista Programador Senior_Full Stack_(2).html\",\n",
    "    r\"1-source-data\\main\\bumeran\\iteration-5\\bandeja-entrada-2022\\Has recibido un CV para el aviso _Analista Programador_(6).html\",\n",
    "    r\"1-source-data\\main\\bumeran\\iteration-5\\programador-net\\Has recibido un CV para el aviso _Programador .Net Senior_(4).html\",\n",
    "  ]\n",
    "\n",
    "  filesFilteredNotIn = [f for f in filesFilteredIsIn if ( f not in filterIsNotIn if filterIsNotIn else True )]\n",
    "\n",
    "  print(\"Archivos filtrados solo con data completa: \" + str(len(filesFilteredNotIn)))\n",
    "\n",
    "  # Limites superiores e inferiores en la búsqueda de archivos\n",
    "  startLimit = None\n",
    "  topLimit = 1000\n",
    "  filesFilteredLimits = filesFilteredNotIn[(startLimit-1 if startLimit else 0): (topLimit if topLimit else len(files))]\n",
    "\n",
    "  return filesFilteredLimits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "21425e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEncodingBumeran(file):\n",
    "  candidateLog = \"\"\n",
    "\n",
    "  encoding = \"windows-1252\"\n",
    "  try:\n",
    "    with open(file, \"r\", encoding=\"utf-16-le\") as f:\n",
    "      if \"charset=unicode\" in f.read():\n",
    "        encoding = 'utf-16-le'\n",
    "      else:\n",
    "        raise Exception\n",
    "  except Exception as e:\n",
    "    try:\n",
    "      with open(file, \"r\", encoding=\"windows-1252\") as f:\n",
    "        if \"charset=windows-1252\" in f.read():\n",
    "          encoding = 'windows-1252'\n",
    "        else:\n",
    "          encoding = encoding\n",
    "    except Exception as e:\n",
    "      candidateLog = candidateLog + file + \"\\n\"\n",
    "      candidateLog = candidateLog + traceback.format_exc() + \"\\n\"\n",
    "      candidateLog = candidateLog + \"\\n\"\n",
    "      pass\n",
    "      \n",
    "  return encoding, candidateLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "c6d1455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCandidateBumeran(file, encoding, candidateLog):\n",
    "  candidateData = {}\n",
    " \n",
    "  try:\n",
    "  # Abriendo el archivo\n",
    "    with open(file, \"r\", encoding=encoding) as myFile:\n",
    "      #Parseando el archivo html a soup\n",
    "      soup = BeautifulSoup(myFile.read(), \"lxml\")\n",
    "\n",
    "      # Obteniendo cada uno de los campos para la data\n",
    "\n",
    "      # Fecha de postulación\n",
    "      spanDates = soup.find_all(\"span\", {\"style\": \"color:black\"})\n",
    "      if len(spanDates) > 4:\n",
    "        rawPostulationDate = spanDates[3].text\n",
    "        postulationDate = datetime.datetime.strptime(spanDates[3].text, \"%A, %B %d, %Y %I:%M %p\")\n",
    "      else:\n",
    "        spanDates2 = soup.find_all(\"span\", {\"style\": 'font-family:\"Calibri\",sans-serif;color:black'})\n",
    "        rawPostulationDate = spanDates2[3].text\n",
    "        postulationDate = datetime.datetime.strptime(spanDates2[3].text, \"%A, %B %d, %Y %I:%M %p\")\n",
    "        \n",
    "      candidateData[\"candidatePostulationDate\"] = postulationDate.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "      # Definiendo los colores a usar para las búsquedas según las fechas\n",
    "      # En la fecha 2020-06-11 cambio el color de los titulos de cada sección\n",
    "      inflexionDate = datetime.datetime(2020,6,11,12,5,0)\n",
    "      colorTitle, colorSections = (\"#008599\", \"#2192C9\") if postulationDate <= inflexionDate else (\"#E90066\", \"#0A26EE\")\n",
    "\n",
    "      # Nombre del perfil\n",
    "      #candidateData[\"jobProfileName\"] = parseNames(parseLineBreaksAndAccents(soup.findAll(\"span\", {\"style\": (\"color:\" + colorTitle) })[0].text))\n",
    "\n",
    "      liTags = soup.find_all(\"ul\", {\"type\": \"disc\"})[0].find_all(\"li\")\n",
    "\n",
    "      # Nombre del candidato\n",
    "      firstLinePased = parseNames(parseLineBreaksAndAccents(liTags[0].text))\n",
    "      candidateData[\"candidateFullName\"] = firstLinePased\n",
    "      \n",
    "      \"\"\"\n",
    "      expectedCivilStatusValues = [\"Soltero/A\", \"Casado/A\", \"Divorciado/A\", \"Pareja De Hecho\", \"Viudo/A\", \"Union Libre\"]\n",
    "      secondLineParsed = parseNames(parseLineBreaksAndAccents(liTags[1].text))\n",
    "      candidateData[\"secondLineParsed\"] = secondLineParsed\n",
    "      \n",
    "      # Pais de residencia\n",
    "      firstcommaIndex = secondLineParsed.find(\",\")\n",
    "      if firstcommaIndex != -1:\n",
    "        candidateData[\"candidateResidenceCountry\"] = secondLineParsed[:firstcommaIndex]\n",
    "      else:\n",
    "        candidateData[\"candidateResidenceCountry\"] = \"\"\n",
    "\n",
    "      # Estado civil\n",
    "      civilStatus = \"\"\n",
    "      for value in expectedCivilStatusValues:\n",
    "        if value in secondLineParsed:\n",
    "          civilStatus = value\n",
    "      candidateData[\"candidateCivilStatus\"] = civilStatus\n",
    "\n",
    "      # Numero de documento\n",
    "      # Propiedad especial del rfind, el -1 coincide con el inicio del string, por lo que es lo mismo tomarlo con o sin el indice, en caso el index no sea -1\n",
    "      lastSpaceIndex = secondLineParsed.rfind(\" \")\n",
    "      candidateData[\"candidateDocumentNumber\"] = re.sub(\"[^0-9]\", \"\", secondLineParsed[lastSpaceIndex+1:])\n",
    "      \n",
    "\n",
    "      # Fecha de nacimiento\n",
    "      thirdLineParsed = parseNames(parseLineBreaksAndAccents(liTags[2].text))\n",
    "      birthDate = thirdLineParsed[thirdLineParsed.find(\":\")+2:]\n",
    "      if birthDate != \"\":\n",
    "        candidateData[\"candidateBirthDate\"] = datetime.datetime.strptime(thirdLineParsed[thirdLineParsed.find(\":\")+2:], \"%d-%m-%Y\").strftime(\"%Y-%m-%d\")\n",
    "      else:\n",
    "        candidateData[\"candidateBirthDate\"] = \"\"\n",
    "\n",
    "      # Pais de nacimiento\n",
    "      fourthLineParsed = parseNames(parseLineBreaksAndAccents(liTags[3].text))\n",
    "      candidateData[\"candidateBirthCountry\"] = fourthLineParsed[fourthLineParsed.find(\":\")+2:]\n",
    "\n",
    "      #Dirección\n",
    "      fifthLineParsed = parseNames(parseLineBreaksAndAccents(liTags[4].text))\n",
    "      candidateData[\"candidateAddress\"] = fifthLineParsed\n",
    "      \n",
    "      sixthLineParsed = parseNames(parseLineBreaksAndAccents(liTags[5].text))\n",
    "      #candidateData[\"contactNumber\"] = sixthLineParsed\n",
    "      #candidateData[\"slash\"] = sixthLineParsed\n",
    "      #candidateData[\"regex\"] = re.sub(\"[^0-9/]\", \"\", sixthLineParsed)\n",
    "      \n",
    "      candidateData[\"homeNumber\"] = sixthLineParsed[sixthLineParsed.find(\"Tel.\")+5:sixthLineParsed.find(\"/\")]\n",
    "      candidateData[\"cellphoneNumber\"] = sixthLineParsed[sixthLineParsed.find(\"/\")+2:]\n",
    "\n",
    "      sixthLineParsed = parseNames(parseLineBreaksAndAccents(liTags[6].text))\n",
    "      candidateData[\"email\"] = sixthLineParsed\n",
    "      \"\"\"\n",
    "\n",
    "      mainDivTag = soup.find_all(\"div\", {\"style\": 'background-position-x:50%;background-position-y:100%'})[0]\n",
    "      mainChildTags = mainDivTag.find_all(recursive=False)\n",
    "      #print(len(mainChildTags))\n",
    "      sectionsIndexes = getSectionsIndexes(mainChildTags, colorSections)\n",
    "      #print(sectionsIndexes)\n",
    "\n",
    "      # Objetivo laboral, salario\n",
    "      \"\"\"\n",
    "      startIndex, endIndex = getStartAndEndIndex(sectionsIndexes, 0)\n",
    "      if startIndex and endIndex:\n",
    "        if (endIndex - startIndex) == 3:\n",
    "          workObjectiveTag = mainChildTags[startIndex+2]\n",
    "          candidateData[\"workObjetive\"] = \"\"\n",
    "          candidateData[\"salary\"] = -1\n",
    "          liTags = workObjectiveTag.find_all(\"li\")\n",
    "          for liTag in liTags:\n",
    "            parsedText = parseNames(parseLineBreaksAndAccents(liTag.text))\n",
    "            if \"Objetivo Laboral:\" in parsedText:\n",
    "              candidateData[\"workObjetive\"] = parsedText[parsedText.find(\"Objetivo Laboral: \")+18:]\n",
    "            if \"Sueldo Pretendido:\" in parsedText:\n",
    "              candidateData[\"salary\"] = int(parsedText[parsedText.find(\"$\")+1:parsedText.find(\".\")])\n",
    "        else:\n",
    "          candidateData[\"workObjetive\"] = \"\"\n",
    "          candidateData[\"salary\"] = -1\n",
    "      else:\n",
    "        candidateData[\"workObjetive\"] = \"\"\n",
    "        candidateData[\"salary\"] = -1\n",
    "      \"\"\"\n",
    "      \n",
    "      # Experiencia laboral\n",
    "      startIndex, endIndex = getStartAndEndIndex(sectionsIndexes, 1)\n",
    "\n",
    "      if startIndex and endIndex:\n",
    "        workExperienceTags = mainChildTags[startIndex+2:endIndex]\n",
    "        firstRowTag = workExperienceTags[0].find(\"span\")\n",
    "        inner_text = [element for element in firstRowTag if isinstance(element, NavigableString)]\n",
    "\n",
    "        print(inner_text)\n",
    "        print(firstRowTag.findAll(text=True))\n",
    "\n",
    "        #firstRowText = parseNames(parseLineBreaksAndAccents(workExperienceTags[0].text))\n",
    "        #candidateData[\"data\"] = firstRowText\n",
    "        \"\"\"candidateData[\"lastWorkArea\"] = firstRowText[find_nth_right(firstRowText, \".\", 2):]\n",
    "        candidateData[\"lastWorkCountry\"] = firstRowText[find_nth_right(firstRowText, \".\", 3):find_nth_right(firstRowText, \".\", 2)]\n",
    "        candidateData[\"lastWorkName\"] = parseNames(parseLineBreaksAndAccents(workExperienceTags[0].find_all(\"b\")[0].text))\n",
    "        candidateData[\"lastWorkPosition\"] = parseNames(parseLineBreaksAndAccents(workExperienceTags[1].find_all(\"b\")[0].text))\"\"\"\n",
    "        \n",
    "        \"\"\"daysOfExperience = 0\n",
    "        for index in range(0, len(workExperienceTags), 2):\n",
    "          startDate = datetime.datetime.strptime(parseLineBreaksAndAccents(workExperienceTags[index].text)[0: parseLineBreaksAndAccents(workExperienceTags[index].text).find(\" - \")] , \"%d-%m-%Y\")\n",
    "          endDateText = parseLineBreaksAndAccents(workExperienceTags[index].text)[parseLineBreaksAndAccents(workExperienceTags[index].text).find(\" - \")+3: parseLineBreaksAndAccents(workExperienceTags[index].text).find(\" | \")]\n",
    "          endDate = datetime.datetime.strptime(endDateText, \"%d-%m-%Y\") if endDateText != \"Presente\" else datetime.datetime.strptime(rawPostulationDate, \"%A, %B %d, %Y %I:%M %p\")\n",
    "          daysOfExperience = daysOfExperience + (endDate - startDate).days\n",
    "        candidateData[\"yearsOfExperience\"] = int(daysOfExperience/365)\n",
    "        candidateData[\"worksNumber\"] = int(len(workExperienceTags)/2)\"\"\"\n",
    "\n",
    "        \"\"\"if (candidateData[\"lastWorkName\"] == \"\" or candidateData[\"lastWorkPosition\"] == \"\"):\n",
    "          raise Exception(\"Error no mapeado (experiencia laboral)\")\"\"\"\n",
    "      else:\n",
    "        raise Exception(\"561 errores mapeados (experiencia laboral)\")\n",
    "        candidateData[\"lastWorkName\"] = candidateData[\"lastWorkPosition\"] = \"\"\n",
    "        candidateData[\"yearsOfExperience\"] = candidateData[\"worksNumber\"] = -1\n",
    "        \n",
    "      \n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      # Nombre del postulante\n",
    "      \"\"\"rawcandidateName = [0].find_all(\"span\")[0].text # El split join tambien quita saltos de línea\n",
    "      candidateData[\"candidateName\"] = parseNames(parseLineBreaksAndAccents(rawcandidateName))\n",
    "\n",
    "      candidateData[\"contactData\"] = str(soup.find_all(\"ul\", {\"type\": \"disc\"})[0])\"\"\"\n",
    "\n",
    "      # Pais de residencia\n",
    "      \"\"\"tagResidenceCountry = soup.find_all(\"ul\", {\"type\": \"disc\"})[0].find_all(\"span\")\n",
    "      if len(tagResidenceCountry) > 2:\n",
    "        if tagResidenceCountry[2].text.find(\",\") != -1:\n",
    "          residenceCountry = parseNames(parseLineBreaksAndAccents(tagResidenceCountry[2].text[0:tagResidenceCountry[2].text.find(\",\")]))\n",
    "          candidateData[\"residenceCountry\"] = residenceCountry\n",
    "          if residenceCountry == \"\":\n",
    "            raise Exception(\"Error no mapeado 3\")\n",
    "        else:\n",
    "          raise Exception(\"Error no mapeado 2\")\n",
    "      else:\n",
    "        raise Exception(\"Error no mapeado 1 (posiblemente data incompleta como los 3 casos)\")\n",
    "      \n",
    "      \n",
    "      mainDivTag = soup.find_all(\"div\", {\"style\": 'background-position-x:50%;background-position-y:100%'})[0]\n",
    "      mainChildTags = mainDivTag.find_all(recursive=False)\n",
    "\n",
    "      sectionsIndexes = getSectionsIndexes(mainChildTags, colorSections)\n",
    "      \n",
    "      # Experiencia laboral\n",
    "      startIndex, endIndex = getStartAndEndIndex(sectionsIndexes, 0)\n",
    "\n",
    "      if startIndex and endIndex:\n",
    "        workExperienceTags = mainChildTags[startIndex+2:endIndex]\n",
    "        candidateData[\"lastWorkCenter\"] = parseNames(parseLineBreaksAndAccents(workExperienceTags[0].find_all(\"b\")[0].text))\n",
    "        candidateData[\"lastWorkPosition\"] = parseNames(parseLineBreaksAndAccents(workExperienceTags[1].find_all(\"b\")[0].text))\n",
    "\n",
    "        daysOfExperience = 0\n",
    "        for index in range(0, len(workExperienceTags), 2):\n",
    "          startDate = datetime.datetime.strptime(parseLineBreaksAndAccents(workExperienceTags[index].text)[0: parseLineBreaksAndAccents(workExperienceTags[index].text).find(\" - \")] , \"%d-%m-%Y\")\n",
    "          endDateText = parseLineBreaksAndAccents(workExperienceTags[index].text)[parseLineBreaksAndAccents(workExperienceTags[index].text).find(\" - \")+3: parseLineBreaksAndAccents(workExperienceTags[index].text).find(\" | \")]\n",
    "          endDate = datetime.datetime.strptime(endDateText, \"%d-%m-%Y\") if endDateText != \"Presente\" else datetime.datetime.strptime(rawPostulationDate, \"%A, %B %d, %Y %I:%M %p\")\n",
    "          daysOfExperience = daysOfExperience + (endDate - startDate).days\n",
    "        candidateData[\"yearsOfExperience\"] = int(daysOfExperience/365)\n",
    "        candidateData[\"worksNumber\"] = int(len(workExperienceTags)/2)\n",
    "\n",
    "        if (candidateData[\"lastWorkCenter\"] == \"\" or candidateData[\"lastWorkPosition\"] == \"\"):\n",
    "          raise Exception(\"Error no mapeado (experiencia laboral)\")\n",
    "      else:\n",
    "        raise Exception(\"561 errores mapeados (experiencia laboral)\")\n",
    "      \n",
    "      \n",
    "      # Carrera profesional (última alcanzada)\n",
    "      startIndex, endIndex = getStartAndEndIndex(sectionsIndexes, 1)\n",
    "\n",
    "      if startIndex and endIndex:\n",
    "        educationTags = mainChildTags[startIndex+2:endIndex]\n",
    "        \n",
    "        careerTags0 = educationTags[0].find_all(\"b\")\n",
    "        candidateData[\"studyCenter\"] = parseNames(parseLineBreaksAndAccents(careerTags0[0].text))\n",
    "        candidateData[\"careerField\"] = parseNames(parseLineBreaksAndAccents(careerTags0[1].text))\n",
    "\n",
    "        careerTags1 = educationTags[1].find_all(\"span\")\n",
    "        tempText = careerTags1[len(careerTags1)-2].text\n",
    "        candidateData[\"careerStatus\"] = parseNames(parseLineBreaksAndAccents(tempText[tempText.find(\",\")+2:tempText.rfind(\",\")]))\n",
    "        candidateData[\"careerDegree\"] = parseNames(parseLineBreaksAndAccents(tempText[tempText.rfind(\",\")+2:tempText.rfind(\".\")]))\n",
    "        candidateData[\"studiesNumber\"] = int(len(educationTags)/2)\n",
    "\n",
    "        if (candidateData[\"studyCenter\"] == \"\" or candidateData[\"careerField\"] == \"\" or candidateData[\"careerStatus\"] == \"\" or candidateData[\"careerDegree\"] == \"\"):\n",
    "          raise Exception(\"Error no mapeado (educación)\")\n",
    "      else:\n",
    "        raise Exception(\"121 errores mapeados (educación)\")\n",
    "\n",
    "      \n",
    "      # Habilidades técnicas\n",
    "      startIndex, endIndex = getStartAndEndIndex(sectionsIndexes, 2)\n",
    "\n",
    "      if startIndex and endIndex:\n",
    "        # Falta mejorar la validación (algunos tienen solo u, solo carrera o ninguno)\n",
    "        technicalSkillsTags = mainChildTags[startIndex+2:endIndex][0].find_all(\"span\")\n",
    "        candidateData[\"technicalSkills\"] = int(len(technicalSkillsTags)/4)\n",
    "      else:\n",
    "        raise Exception(\"Errores no mapeados (habilidades técnicas)\")\n",
    "\n",
    "      # Lenguajes\n",
    "      startIndex, endIndex = getStartAndEndIndex(sectionsIndexes, 3)\n",
    "\n",
    "      if startIndex and endIndex:\n",
    "        # Falta mejorar la validación (algunos tienen solo u, solo carrera o ninguno)\n",
    "        languagesTags = mainChildTags[startIndex+2:endIndex][0].find_all(\"span\")\n",
    "        candidateData[\"languages\"] = int(len(languagesTags)/7)\n",
    "      else:\n",
    "        raise Exception(\"Errores no mapeados (lenguajes)\")\n",
    "\n",
    "      # Otros conocimientos (habilidades blandas)\n",
    "      startIndex, endIndex = getStartAndEndIndex(sectionsIndexes, 4)\n",
    "\n",
    "      if startIndex and endIndex:\n",
    "        # Falta mejorar la validación (algunos tienen solo u, solo carrera o ninguno)\n",
    "        otherKnowledgesTags = mainChildTags[startIndex+2:endIndex][0].find_all(\"span\")\n",
    "        candidateData[\"anotherSkills\"] = int(len(otherKnowledgesTags)/3)\n",
    "      else:\n",
    "        raise Exception(\"Errores no mapeados (otras habilidades)\")\n",
    "\n",
    "      # Pendiente de analizar casos fraccionarios (se trunco para caso práctico)\n",
    "\n",
    "      # Salario pretendido\n",
    "      tagsSalary = [index for index, tag in enumerate(soup.find_all(\"span\")) if \"Sueldo pretendido\" in tag.text]\n",
    "      if len(tagsSalary) > 0:\n",
    "        rawSalary = soup.find_all(\"span\")[tagsSalary[0]+1].text\n",
    "        candidateData[\"salary\"] = int(rawSalary[rawSalary.find(\"$\")+1:rawSalary.find(\".\")])\n",
    "      else:\n",
    "        raise Exception(\"Errores no mapeados (salario)\")\n",
    "        \"\"\"\n",
    "\n",
    "  except Exception as e:\n",
    "    candidateData = {}\n",
    "    candidateLog = candidateLog + str(file) + \"\\n\"\n",
    "    candidateLog = candidateLog + traceback.format_exc() + \"\\n\"\n",
    "    candidateLog = candidateLog + \"\\n\"\n",
    "    pass\n",
    "\n",
    "  return candidateData, candidateLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "2ec7e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(Thread):\n",
    "  def __init__(self, queue, filesCount):\n",
    "    Thread.__init__(self)\n",
    "    self.queue = queue\n",
    "    self.filesCount = filesCount\n",
    "    self.candidateData = []\n",
    "    self.candidateLog = []\n",
    "\n",
    "  def run(self):\n",
    "    while True:\n",
    "      elem = self.queue.get()\n",
    "      if not elem:\n",
    "        break\n",
    "      \n",
    "      index, file = elem[\"index\"], elem[\"file\"]\n",
    "\n",
    "      if index % 500 == 0:\n",
    "        print(str(index) + \"/\" + str(self.filesCount) + \" archivos analizados\")\n",
    "      \n",
    "      # Obteniendo el encoding por cada archivo\n",
    "      encoding, candidateLog = getEncodingBumeran(file)\n",
    "\n",
    "      # Obteniendo los datos por cada archivo\n",
    "      candidateData, candidateLog = getCandidateBumeran(file, encoding, candidateLog)\n",
    "      \n",
    "      if candidateData:\n",
    "        self.candidateData.append(candidateData)\n",
    "      if candidateLog:\n",
    "        self.candidateLog.append(candidateLog)\n",
    "\n",
    "      self.queue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "9ac7ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateCandidateQueue(files, workersNumber):\n",
    "  filesCount = len(files)\n",
    "\n",
    "  q = queue.Queue()\n",
    "  for index, file in enumerate(files):\n",
    "    q.put({\"index\": index, \"file\": file})\n",
    "  \n",
    "  for _ in range(workersNumber):\n",
    "    q.put({})\n",
    "  \n",
    "  workers = []\n",
    "  for _ in range(workersNumber):\n",
    "      worker = Worker(q, filesCount)\n",
    "      worker.start()\n",
    "      workers.append(worker)\n",
    "  \n",
    "  for worker in workers:\n",
    "      worker.join()\n",
    "  \n",
    "  data = []\n",
    "  log = \"\"\n",
    "  for worker in workers:\n",
    "    data.extend(worker.candidateData)\n",
    "    log = log + \"\".join(worker.candidateLog)\n",
    "\n",
    "  return data, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "472c3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAndWriteMain(source):\n",
    "  # Definiendo la carpeta raiz de los archivos a buscar, según el origen\n",
    "  rootPath = bumeranRootPath\n",
    "\n",
    "  # Obteniendo la lista de archivos, según el origen\n",
    "  files = getFiles(rootPath, source)\n",
    "\n",
    "  # Definiendo el numero de hilos\n",
    "  workersNumber = 1\n",
    "\n",
    "  # Obteniendo los datos de candidatos y el log\n",
    "  data, log = iterateCandidateQueue(files, workersNumber)\n",
    "\n",
    "  # Realizando un ordenamiento por fecha de postulación, en orden descendente\n",
    "  #data = sorted(data, key=lambda x: (x[\"candidatePostulationDate\"]), reverse=True)\n",
    "\n",
    "  # Escribiendo la data de los candidatos (json y csv)\n",
    "  writeJson(data, os.path.join(intermFilesFolder, source + '.json'))\n",
    "  writeCsv(data, os.path.join(intermFilesFolder, source + '.csv'))\n",
    "\n",
    "  # Escribiendo el log de errores\n",
    "  writeTxt(log, os.path.join(logsFolder, source, datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\") + \".txt\"), 'utf-16')\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "a8528b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAndWriteMergedMain(mainData):\n",
    "  mergedMainData = []\n",
    "  for elem in mainData:\n",
    "    if (elem):\n",
    "      mergedMainData.extend(elem)\n",
    "\n",
    "  writeJson(mergedMainData, os.path.join(mergedMainFolder, \"result.json\"), 'utf-8')\n",
    "  writeCsv(mergedMainData, os.path.join(mergedMainFolder, \"result.csv\"), 'utf-8')\n",
    "\n",
    "  return mergedMainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "2925ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeData(mergedMainData):\n",
    "  df = pd.DataFrame(mergedMainData)\n",
    "\n",
    "  print(df.count())\n",
    "\n",
    "  columns = [elem for elem in df.columns if elem not in [\"candidatePostulationDate\", \"candidateFullName\"]]\n",
    "\n",
    "  for column in columns:\n",
    "    topDf = df[column].value_counts()[:dataVisualizationTopLimit]\n",
    "    print(topDf)\n",
    "    y_axis = list(reversed(topDf.index))\n",
    "    x_axis = list(reversed(topDf.values))\n",
    "    plt.ylabel(column)\n",
    "    plt.barh(y_axis, x_axis)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "c9021c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio: 2023-05-19 03:11:24.375878\n",
      "Se inició el procesamiento\n",
      "Archivos originales: 10230\n",
      "Archivos filtrados solo con postulaciones: 10228\n",
      "Archivos filtrados solo con data completa: 1\n",
      "0/1 archivos analizados\n",
      "['01-04-2019 - Presente |\\n', ' Peru. Programación. ']\n",
      "['01-04-2019 - Presente |\\n', 'Global Hitss', ' Peru. Programación. ']\n",
      "Se terminó de procesar Bumeran\n",
      "Se terminó de unir la data principal\n",
      "candidatePostulationDate    1\n",
      "candidateFullName           1\n",
      "dtype: int64\n",
      "Fin: 2023-05-19 03:11:24.981765\n",
      "Tiempo: 0:00:00.605887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ronaldo\\AppData\\Local\\Temp\\ipykernel_16264\\498708785.py:129: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  print(firstRowTag.findAll(text=True))\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "  # Definiendo el inicio del proceso\n",
    "  startTime = datetime.datetime.now()\n",
    "  print(\"Inicio: \" + str(startTime))\n",
    "  print(\"Se inició el procesamiento\")\n",
    "\n",
    "  # El dataset principal que sea el de bumeran\n",
    "  # Los otros archivos que solo se usen para actualizar la variable objetivo\n",
    "  # Esto porque los otros orígenes están incompletos y costaría llenar los campos (incluso el excel que se hizo)\n",
    "  isLoadedBumeran = False\n",
    "  isMergedMain = False\n",
    "  #False\n",
    "\n",
    "  # Leyendo o calculando bumeran\n",
    "  bumeranData = readJson(os.path.join(intermFilesFolder, 'bumeran.json')) if isLoadedBumeran else readAndWriteMain('bumeran')\n",
    "  print(\"Se terminó de procesar Bumeran\")\n",
    "\n",
    "  # Uniendo la data principal (bumeran + linkedin (ya no))\n",
    "  mergedMainData = readJson(os.path.join(mergedMainFolder, 'result.json')) if isMergedMain else readAndWriteMergedMain([bumeranData])\n",
    "  print(\"Se terminó de unir la data principal\")\n",
    "\n",
    "  visualizeData(mergedMainData)\n",
    "  \n",
    "  # Definiendo el fin del proceso\n",
    "  endTime = datetime.datetime.now()\n",
    "  print(\"Fin: \" + str(endTime))\n",
    "  print(\"Tiempo: \" + str(endTime-startTime))\n",
    "\n",
    "  #print(counterIn)\n",
    "  #print(counterOut)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
