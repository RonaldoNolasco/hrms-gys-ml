{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dad0ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, confusion_matrix, accuracy_score, recall_score\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "763cd073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "inputFolder = \"1-input\"\n",
    "processFolder = \"2-process\"\n",
    "outputFolder = \"3-output\"\n",
    "logsFolder = \"4-logs\"\n",
    "\n",
    "outputAlgorithmsFolder = outputFolder + r\"\\algorithms\"\n",
    "outputCrossValidationFolder = outputFolder + r\"\\cross-validation\"\n",
    "\n",
    "dataVisualizationTopLimit = 20\n",
    "\n",
    "testSize = 0.25\n",
    "randomState = 0\n",
    "partitionsNumber = 5\n",
    "samplingStrategy = 0.2 # Arreglar\n",
    "percentileNumber = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "919ac9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilitarias\n",
    "def readData(filePath, delimiter, encoding, header):\n",
    "  data = pd.read_csv(filePath, delimiter=delimiter,encoding=encoding, header=header)\n",
    "  return data\n",
    "\n",
    "def writeJson(data, pathJson, encoding='utf-8'):\n",
    "  with open(pathJson, 'w', encoding=encoding) as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def writeCsv(data, pathCsv, encoding='utf-8'):\n",
    "  with open(pathCsv, 'w', newline='', encoding=encoding) as f:\n",
    "    if data:\n",
    "      writer = csv.DictWriter(f, fieldnames=data[0].keys(), lineterminator='\\n')\n",
    "      writer.writeheader()\n",
    "      writer.writerows(data)\n",
    "    else:\n",
    "      f.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "311c6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingData(df):\n",
    "  # Aca no se realizará ni parseo ni nada, ya vendrá desde la fuente\n",
    "  # Solo se leerá, se dividirá y se entrenará\n",
    "\n",
    "  # Balanceo de datos: Sobremuestreo aleatorio (oversampling)\n",
    "  # De cada 5 no contratados habrá un contratado\n",
    "  objectiveColumn = \"contratado\"\n",
    "  dictResults = dict(df[objectiveColumn].value_counts().sort_index())\n",
    "  #print(dictResults)\n",
    "\n",
    "  maxKey = max(dictResults, key=dictResults.get)\n",
    "  maxValue = max(dictResults.values())\n",
    "  #print(maxKey)\n",
    "  #print(maxValue)\n",
    "\n",
    "  dfClassMaxKey = df[df[objectiveColumn] == maxKey]\n",
    "\n",
    "  for key, value in dictResults.items():\n",
    "    if key != maxKey:\n",
    "      dfClass = df[df[objectiveColumn] == key]\n",
    "      dfClassSampled = dfClass.sample(int(maxValue * samplingStrategy), random_state=randomState, replace=True)\n",
    "      dfClassMaxKey = pd.concat([dfClassMaxKey, dfClassSampled],axis=0)\n",
    "\n",
    "  df = dfClassMaxKey\n",
    "\n",
    "  # Aleatorizacion del orden de los registros para evitar sesgos(filas)\n",
    "  df = df.sample(frac = 1, random_state=randomState).reset_index(drop=True)  \n",
    "\n",
    "  # Aplicando OrdinalEncoding a las variables categóricas ordinales()\n",
    "  categoricalColumns = [columnName for columnName, columnType in df.dtypes.to_dict().items() if columnName not in [ \"contratado\" ] and columnType == \"object\" ]\n",
    "  categoricalOrdinalColumns = [columnName for columnName in categoricalColumns if columnName in [ \"estadoUltimoEstudio\", \"gradoUltimoEstudio\" ]]\n",
    "\n",
    "  encoder = OrdinalEncoder(categories=[[ \"Abandonado\", \"En Curso\", \"Graduado\" ]])\n",
    "  encoder.fit(df[[\"estadoUltimoEstudio\"]])\n",
    "  df[\"estadoUltimoEstudio\"] = encoder.transform(df[[\"estadoUltimoEstudio\"]])\n",
    "\n",
    "  encoder = OrdinalEncoder(categories=[[ \"Otro\", \"Secundario\", \"Terciario/Tecnico\", \"Universitario\", \"Posgrado\", \"Master\", \"Doctorado\" ]])\n",
    "  encoder.fit(df[[\"gradoUltimoEstudio\"]])\n",
    "  df[\"gradoUltimoEstudio\"] = encoder.transform(df[[\"gradoUltimoEstudio\"]])\n",
    "\n",
    "  #display(df)\n",
    "\n",
    "  # Aplicando OneHotEncoding a las variables categóricas cardinales (transformación a numéricas mediante columnas)\n",
    "  categoricalCardinalColumns = [columnName for columnName in categoricalColumns if columnName not in [ \"estadoUltimoEstudio\", \"gradoUltimoEstudio\" ]]\n",
    "  for column in categoricalCardinalColumns:\n",
    "    dummies = pd.get_dummies(df[[column]], prefix=column, dummy_na=True)\n",
    "    df = pd.concat([df, dummies], axis = 1)\n",
    "    df = df.drop(columns=[column])\n",
    "\n",
    "  #display(df.dtypes)\n",
    "\n",
    "  # Aplicando MinMaxScaler a las variables numéricas (normalización) (esto tambien incluye a lastEducationStatus y lastEducationDegree, ya numéricas)\n",
    "  # Algunas quedaran en 0.9999, esto porque no todas manejan la misma escala (sin decimales, o solo un decimal)\n",
    "  numericalColumns = [columnName for columnName, columnType in df.dtypes.to_dict().items() if columnName not in [ \"contratado\" ] and columnType == \"float64\" ]\n",
    "  for column in numericalColumns:\n",
    "    df[column] = df[column].fillna(0.0)\n",
    "  mms = MinMaxScaler()\n",
    "  df[numericalColumns] = mms.fit_transform(df[numericalColumns])\n",
    "\n",
    "  # Leyendo el numero de atributos\n",
    "  # 19146 filas, 12403 columnas con el drop_first=True\n",
    "  # 19146 filas, 12415 columnas sin el drop_first=True\n",
    "\n",
    "  # Eliminando columnas con varianza cercana a cero (variables no afectan en el resultado del modelo)\n",
    "  df.loc['std'] = df.std()\n",
    "  stdArray = df.iloc[len(df)-1]\n",
    "  ninetyNinthPercentile = np.percentile(stdArray, percentileNumber)\n",
    "  df = df.transpose()\n",
    "  df = df[df[\"std\"]>ninetyNinthPercentile]\n",
    "  df = df.transpose()\n",
    "  df = df.drop(['std'], axis=0)\n",
    "\n",
    "  # Lectura de las variables de características y objetivo\n",
    "  objectiveColumn = \"contratado\"\n",
    "  X = df.drop([objectiveColumn], axis=1)\n",
    "  y = df[objectiveColumn]\n",
    "\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49ddd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X, y, partitionNumber):\n",
    "  # Dividiendo los dataframes de entrenamiento y prueba\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = testSize, random_state=randomState)\n",
    "\n",
    "  if partitionNumber != 0:\n",
    "    # Obteniendo el total de filas del dataframe de testeo\n",
    "    X_train, X_test, y_train, y_test = X_train.reset_index(drop=True), X_test.reset_index(drop=True), y_train.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "    totalRows = len(X_train)\n",
    "\n",
    "    # Determinando el límite inferior (primera fila) de la partición\n",
    "    bottomLimit = int(totalRows*(1/partitionsNumber)*(partitionNumber-1)) + 1\n",
    "\n",
    "    # Determinando el límite superior (última fila) de la partición\n",
    "    topLimit = int(totalRows*(1/partitionsNumber)*partitionNumber)\n",
    "    #print(bottomLimit, topLimit)\n",
    "\n",
    "    # Determinando una lista con todas los numeros de las filas con la partición\n",
    "    partitionList = [x for x in range(bottomLimit-1, topLimit)]\n",
    "\n",
    "    # Determinando los dataframes de las categorías\n",
    "    X_train, X_test = X_train[~X_train.index.isin(partitionList)], X_train[X_train.index.isin(partitionList)]\n",
    "\n",
    "    # Determinando los dataframes de los objetivos\n",
    "    y_train, y_test = y_train[~y_train.index.isin(partitionList)], y_train[y_train.index.isin(partitionList)]\n",
    "\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18fae5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createClassifier(modelName):\n",
    "  if modelName == \"KNN\":\n",
    "    return KNeighborsClassifier()\n",
    "  elif modelName == \"LR\":\n",
    "    return LogisticRegression(random_state=randomState, max_iter=200)\n",
    "  elif modelName == \"GNB\":\n",
    "    return GaussianNB()\n",
    "  elif modelName == \"DT\":\n",
    "    return DecisionTreeClassifier(random_state=randomState)\n",
    "  elif modelName == \"SVM\":\n",
    "    return SVC(random_state=randomState)\n",
    "  elif modelName == \"RF\":\n",
    "    return RandomForestClassifier(random_state=randomState)\n",
    "  elif modelName == \"GB\":\n",
    "    return GradientBoostingClassifier(random_state=randomState)\n",
    "  else:\n",
    "    return KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2be135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(X_train, X_test, y_train, y_test, modelName):\n",
    "  # Creación del clasificador KNN\n",
    "  clf = createClassifier(modelName)\n",
    "\n",
    "  # Entrenamiento del clasificador KNN\n",
    "  clf.fit(X_train, y_train)\n",
    "\n",
    "  # Calculando la predicción del modelo con la data de prueba\n",
    "  y_pred = clf.predict(X_test)\n",
    "\n",
    "  return y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1887988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(y_train, y_test, y_pred, startDate, endDate, partitionNumber, algorithm = \"KNN\"):\n",
    "  trainRows = len(y_train)\n",
    "  testRows = len(y_test)\n",
    "\n",
    "  # Calculando la exactitud del modelo\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "  # Calculando la precisión del modelo\n",
    "  precision = precision_score(y_test, y_pred)\n",
    "\n",
    "  # Calculando la sensibilidad del modelo\n",
    "  recall = recall_score(y_test, y_pred)\n",
    "\n",
    "  # Calculando el valor F del modelo (robustez)\n",
    "  f1Score = f1_score(y_test, y_pred)\n",
    "\n",
    "  # Calculando el promedio de métricas\n",
    "  metricsList = [accuracy, precision, recall, f1Score]\n",
    "  metricsMean = sum(metricsList) / len(metricsList)\n",
    "\n",
    "  # Calculando el tiempo de ejecución del modelo\n",
    "  executionTime = (endDate - startDate).total_seconds()\n",
    "  formatExecutionTime = \"{:.2f}\".format(executionTime) + \"s\"\n",
    "  formatAverageTime = \"{:.2f}\".format(executionTime*1000/(trainRows + testRows)) + \"ms\"\n",
    "\n",
    "  confussionMatrix = str(confusion_matrix(y_test, y_pred).tolist())\n",
    "  \n",
    "  return {\n",
    "    \"algoritmo\": algorithm,\n",
    "    \"particion\": \"Total de datos\" if partitionNumber == 0 else \"Particion \" + str(partitionNumber),\n",
    "    \"registrosEntrenamiento\": trainRows,\n",
    "    \"registrosPrueba\": testRows,\n",
    "    \"proporcionSobremuestreo\": samplingStrategy,\n",
    "    \"tiempoEjecucion\": formatExecutionTime,\n",
    "    \"matrizConfusion\": confussionMatrix,\n",
    "    \"exactitud\": \"{:.2%}\".format(accuracy),\n",
    "    \"precision\": \"{:.2%}\".format(precision),\n",
    "    \"sensibilidad\": \"{:.2%}\".format(recall),\n",
    "    \"robustez\": \"{:.2%}\".format(f1Score),\n",
    "    \"promedioMetricas\": \"{:.2%}\".format(metricsMean),\n",
    "    \"tiempoPromedio\": formatAverageTime\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83f3c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  # Definiendo el inicio del proceso\n",
    "  startTime = datetime.datetime.now()\n",
    "  print(\"Inicio: \" + str(startTime))\n",
    "  print()\n",
    "\n",
    "  # Leyendo la data\n",
    "  data = readData(os.path.join(inputFolder, \"result.csv\"), ',', 'utf-8', 0)\n",
    "\n",
    "  # Determinando los dataframes de las categorías (X) y el objetivo (y)\n",
    "  X, y = preprocessingData(data)\n",
    "\n",
    "  # Definiendo los algoritmos a usar\n",
    "  algorithms = [\"KNN\", \"LR\", \"GNB\", \"DT\", \"SVM\", \"RF\", \"GB\"]\n",
    "\n",
    "  # Creando el arreglo de metricas de cada algoritmo\n",
    "  algorithmsMetricsList = []\n",
    "\n",
    "  for algorithm in algorithms:\n",
    "    # Mostrando que algoritmo se usa\n",
    "    print(\"Ejecutando para algoritmo {}\".format(algorithm))\n",
    "\n",
    "    # Inicio de ejecución\n",
    "    startDate = datetime.datetime.now()\n",
    "    #print(\"Inicio: \" + str(startDate))\n",
    "\n",
    "    # Separando data para el entrenamiento y testeo\n",
    "    X_train, X_test, y_train, y_test = splitData(X, y, 0)\n",
    "\n",
    "    # Realizar entrenamiento del modelo\n",
    "    y_test, y_pred = trainModel(X_train, X_test, y_train, y_test, algorithm)\n",
    "\n",
    "    # Fin de ejecución del modelo\n",
    "    endDate = datetime.datetime.now()\n",
    "    #print(\"Fin: \" + str(endDate))\n",
    "    #print(\"Tiempo: \" + str(endDate-startDate))\n",
    "    #print()\n",
    "\n",
    "    # Obteniendo las métricas de la partición del modelo\n",
    "    algorithmMetrics = getMetrics(y_train, y_test, y_pred, startDate, endDate, 0, algorithm)\n",
    "\n",
    "    # Añadiendo la métrica de la partición a la lista de métricas\n",
    "    algorithmsMetricsList.append(algorithmMetrics)\n",
    "\n",
    "  # Ordenando las métricas\n",
    "  algorithmsMetricsList = sorted(algorithmsMetricsList, key=lambda x: (x[\"algoritmo\"]))\n",
    "\n",
    "  # Escribiendo las metricas en un archivo de salida\n",
    "  outputAlgorithmsDateTime = datetime.datetime.now()\n",
    "  writeCsv(algorithmsMetricsList, os.path.join(outputAlgorithmsFolder, outputAlgorithmsDateTime.strftime(\"%Y-%m-%d %H-%M-%S\") + \".csv\"))\n",
    "\n",
    "  # Elegir el algoritmo con mayor promedio de métricas\n",
    "  maxAverageAlgorithm = max(algorithmsMetricsList, key=lambda x:x[\"promedioMetricas\"])\n",
    "  selectedAlgorithm, selectedAlgorithmAverage = maxAverageAlgorithm[\"algoritmo\"], maxAverageAlgorithm[\"promedioMetricas\"]\n",
    "\n",
    "  print()\n",
    "  print(\"Algoritmo con mayor promedio: {}\".format(selectedAlgorithm))\n",
    "  print(\"Promedio de métricas: {}\".format(selectedAlgorithmAverage))\n",
    "  print()\n",
    "\n",
    "  # Creando el arreglo de metricas de la validación cruzada\n",
    "  crossValidationMetricsList = []\n",
    "\n",
    "  for partitionNumber in range (0, partitionsNumber+1):\n",
    "    # Mostrando el número de la partició\n",
    "    print(\"Ejecutando para {}\".format(\"el total de datos\" if partitionNumber == 0 else (\"la partición {}\".format(partitionNumber))))\n",
    "\n",
    "    # Inicio de ejecución\n",
    "    startDate = datetime.datetime.now()\n",
    "    #print(\"Inicio: \" + str(startDate))\n",
    "\n",
    "    # Separando data para el entrenamiento y testeo\n",
    "    X_train, X_test, y_train, y_test = splitData(X, y, partitionNumber)\n",
    "\n",
    "    # Realizar entrenamiento del modelo\n",
    "    y_test, y_pred = trainModel(X_train, X_test, y_train, y_test, selectedAlgorithm)\n",
    "\n",
    "    # Fin de ejecución del modelo\n",
    "    endDate = datetime.datetime.now()\n",
    "    #print(\"Fin: \" + str(endDate))\n",
    "    #print(\"Tiempo: \" + str(endDate-startDate))\n",
    "    #print()\n",
    "\n",
    "    # Obteniendo las métricas de la partición del modelo\n",
    "    crossValidationMetrics = getMetrics(y_train, y_test, y_pred, startDate, endDate, partitionNumber, selectedAlgorithm)\n",
    "\n",
    "    # Añadiendo la métrica de la partición a la lista de métricas\n",
    "    crossValidationMetricsList.append(crossValidationMetrics)\n",
    "\n",
    "  # Ordenando las métricas\n",
    "  crossValidationMetricsList = sorted(crossValidationMetricsList, key=lambda x: (x[\"particion\"]))\n",
    "\n",
    "  # Escribiendo las metricas en un archivo de salida\n",
    "  outputCrossValidationDateTime = datetime.datetime.now()\n",
    "  writeCsv(crossValidationMetricsList, os.path.join(outputCrossValidationFolder, outputCrossValidationDateTime.strftime(\"%Y-%m-%d %H-%M-%S\") + \".csv\"))\n",
    "\n",
    "  # Elegir la partición con mayor promedio de métricas\n",
    "  maxAveragePartition = max(algorithmsMetricsList, key=lambda x:x[\"promedioMetricas\"])\n",
    "  selectedPartition, selectedPartitionAverage = maxAveragePartition[\"particion\"], maxAveragePartition[\"promedioMetricas\"]\n",
    "\n",
    "  print()\n",
    "  print(\"Partición con mayor promedio: {}\".format(selectedPartition))\n",
    "  print(\"Promedio de métricas: {}\".format(selectedPartitionAverage))\n",
    "  print()\n",
    "  print(\"Exactitud: {}\".format(maxAveragePartition[\"exactitud\"]))\n",
    "  print(\"Precisión: {}\".format(maxAveragePartition[\"precision\"]))\n",
    "  print(\"Sensibilidad: {}\".format(maxAveragePartition[\"sensibilidad\"]))\n",
    "  print(\"Robustez: {}\".format(maxAveragePartition[\"robustez\"]))\n",
    "  print(\"Tiempo promedio: {}\".format(maxAveragePartition[\"tiempoPromedio\"]))\n",
    "  print()\n",
    "\n",
    "  # Definiendo el fin del proceso\n",
    "  endTime = datetime.datetime.now()\n",
    "  print(\"Fin: \" + str(endTime))\n",
    "  print(\"Tiempo: \" + str(endTime-startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c95c684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio: 2023-06-06 02:35:51.374844\n",
      "\n",
      "Ejecutando para algoritmo KNN\n",
      "Ejecutando para algoritmo LR\n",
      "Ejecutando para algoritmo GNB\n",
      "Ejecutando para algoritmo DT\n",
      "Ejecutando para algoritmo SVM\n",
      "Ejecutando para algoritmo RF\n",
      "Ejecutando para algoritmo GB\n",
      "\n",
      "Algoritmo con mayor promedio: RF\n",
      "Promedio de métricas: 98.30%\n",
      "\n",
      "Ejecutando para el total de datos\n",
      "Ejecutando para la partición 1\n",
      "Ejecutando para la partición 2\n",
      "Ejecutando para la partición 3\n",
      "Ejecutando para la partición 4\n",
      "Ejecutando para la partición 5\n",
      "\n",
      "Partición con mayor promedio: Total de datos\n",
      "Promedio de métricas: 98.30%\n",
      "\n",
      "Exactitud: 99.34%\n",
      "Precisión: 100.00%\n",
      "Sensibilidad: 95.94%\n",
      "Robustez: 97.93%\n",
      "Tiempo promedio: 0.45ms\n",
      "\n",
      "Fin: 2023-06-06 02:37:00.999676\n",
      "Tiempo: 0:01:09.624832\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
