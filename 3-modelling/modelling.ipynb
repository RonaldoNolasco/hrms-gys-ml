{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "dad0ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, confusion_matrix, accuracy_score, recall_score\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "763cd073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "inputFolder = \"1-input\"\n",
    "processFolder = \"2-process\"\n",
    "outputFolder = \"3-output\"\n",
    "logsFolder = \"4-logs\"\n",
    "\n",
    "dataVisualizationTopLimit = 20\n",
    "\n",
    "testSize = 0.25\n",
    "randomState = 0\n",
    "partitionsNumber = 4\n",
    "samplingStrategy = 0.2 # Arreglar\n",
    "percentileNumber = 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "919ac9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilitarias\n",
    "def readData(filePath, delimiter, encoding, header):\n",
    "  data = pd.read_csv(filePath, delimiter=delimiter,encoding=encoding, header=header)\n",
    "  return data\n",
    "\n",
    "def writeJson(data, pathJson, encoding='utf-8'):\n",
    "  with open(pathJson, 'w', encoding=encoding) as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def writeCsv(data, pathCsv, encoding='utf-8'):\n",
    "  with open(pathCsv, 'w', newline='', encoding=encoding) as f:\n",
    "    if data:\n",
    "      writer = csv.DictWriter(f, fieldnames=data[0].keys(), lineterminator='\\n')\n",
    "      writer.writeheader()\n",
    "      writer.writerows(data)\n",
    "    else:\n",
    "      f.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "311c6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingData(df):\n",
    "  # Aca no se realizará ni parseo ni nada, ya vendrá desde la fuente\n",
    "  # Solo se leerá, se dividirá y se entrenará\n",
    "\n",
    "  # Balanceo de datos: Sobremuestreo aleatorio (oversampling)\n",
    "  # # De cada 2 no contratados habrá un contratado\n",
    "  objectiveColumn = \"hired\"\n",
    "  dictResults = dict(df[objectiveColumn].value_counts().sort_index())\n",
    "\n",
    "  #print(dictResults)\n",
    "\n",
    "  maxKey = max(dictResults, key=dictResults.get)\n",
    "  maxValue = max(dictResults.values())\n",
    "\n",
    "  #print(maxKey)\n",
    "  #print(maxValue)\n",
    "\n",
    "  dfClassMaxKey = df[df[objectiveColumn] == maxKey]\n",
    "\n",
    "  for key, value in dictResults.items():\n",
    "    if key != maxKey:\n",
    "      dfClass = df[df[objectiveColumn] == key]\n",
    "      dfClassSampled = dfClass.sample(int(maxValue * samplingStrategy), random_state=randomState, replace=True)\n",
    "      dfClassMaxKey = pd.concat([dfClassMaxKey, dfClassSampled],axis=0)\n",
    "\n",
    "  df = dfClassMaxKey\n",
    "  \n",
    "  #print(dict(df[objectiveColumn].value_counts().sort_index()))\n",
    "\n",
    "  # Desordenando la data para evitar sesgos(filas)\n",
    "  df = df.sample(frac = 1, random_state=randomState).reset_index(drop=True)  \n",
    "\n",
    "  # Aplicando OrdinalEncoding a las variables categóricas ordinales()\n",
    "  categoricalColumns = [columnName for columnName, columnType in df.dtypes.to_dict().items() if columnName not in [ \"hired\" ] and columnType == \"object\" ]\n",
    "  categoricalOrdinalColumns = [columnName for columnName in categoricalColumns if columnName in [ \"lastEducationStatus\", \"lastEducationDegree\" ]]\n",
    "\n",
    "  # OrdinalEncoder para la columna lastEducationStatus\n",
    "  encoder = OrdinalEncoder(categories=[[ \"Abandonado\", \"En Curso\", \"Graduado\" ]])\n",
    "  encoder.fit(df[[\"lastEducationStatus\"]])\n",
    "  df[\"lastEducationStatus\"] = encoder.transform(df[[\"lastEducationStatus\"]])\n",
    "\n",
    "  # OrdinalEncoder para la columna lastEducationDegree\n",
    "  encoder = OrdinalEncoder(categories=[[ \"Otro\", \"Secundario\", \"Terciario/Tecnico\", \"Universitario\", \"Posgrado\", \"Master\", \"Doctorado\" ]])\n",
    "  encoder.fit(df[[\"lastEducationDegree\"]])\n",
    "  df[\"lastEducationDegree\"] = encoder.transform(df[[\"lastEducationDegree\"]])\n",
    "\n",
    "  #display(df)\n",
    "\n",
    "  # Aplicando OneHotEncoding a las variables categóricas cardinales (transformación a numéricas mediante columnas)\n",
    "  categoricalCardinalColumns = [columnName for columnName in categoricalColumns if columnName not in [ \"lastEducationStatus\", \"lastEducationDegree\" ]]\n",
    "  for column in categoricalCardinalColumns:\n",
    "    dummies = pd.get_dummies(df[[column]], prefix=column, dummy_na=True)\n",
    "    df = pd.concat([df, dummies], axis = 1)\n",
    "    df = df.drop(columns=[column])\n",
    "\n",
    "  #display(df.dtypes)\n",
    "\n",
    "  # Aplicando MinMaxScaler a las variables numéricas (normalización) (esto tambien incluye a lastEducationStatus y lastEducationDegree, ya numéricas)\n",
    "  # Algunas quedaran en 0.9999, esto porque no todas manejan la misma escala (sin decimales, o solo un decimal)\n",
    "  numericalColumns = [columnName for columnName, columnType in df.dtypes.to_dict().items() if columnName not in [ \"hired\" ] and columnType == \"float64\" ]\n",
    "  #print(numericalColumns)\n",
    "  for column in numericalColumns:\n",
    "    df[column] = df[column].fillna(0.0)\n",
    "\n",
    "  mms = MinMaxScaler()\n",
    "  df[numericalColumns] = mms.fit_transform(df[numericalColumns])\n",
    "\n",
    "  # Leyendo el numero de atributos\n",
    "  # 19146 filas, 12403 columnas con el drop_first=True\n",
    "  # 19146 filas, 12415 columnas sin el drop_first=True\n",
    "\n",
    "  # Revision maximos y minimos (todos si estan entre 0 y 1)\n",
    "  \"\"\"df.loc['max'] = df.max()\n",
    "  df.loc['min']= df.min()\n",
    "  maxValue, minValue = max(list(df.iloc[len(df)-2])), min(list(df.iloc[len(df)-1]))\n",
    "  print(maxValue, minValue)\n",
    "  df = df.drop(['max', 'min'], axis=0)\"\"\"\n",
    "\n",
    "  # Eliminando columnas con varianza cercana a cero (variables no afectan en el resultado del modelo)\n",
    "  df.loc['std'] = df.std()\n",
    "  stdArray = df.iloc[len(df)-1]\n",
    "  ninetyNinthPercentile = np.percentile(stdArray, percentileNumber)\n",
    "  df = df.transpose()\n",
    "  df = df[df[\"std\"]>ninetyNinthPercentile]\n",
    "  df = df.transpose()\n",
    "  df = df.drop(['std'], axis=0)\n",
    "\n",
    "  # Lectura de las variables de características y objetivo\n",
    "  objectiveColumn = \"hired\"\n",
    "  X = df.drop([objectiveColumn], axis=1)\n",
    "  y = df[objectiveColumn]\n",
    "\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "49ddd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X, y, partitionNumber):\n",
    "  # Dividiendo los dataframes de entrenamiento y prueba\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = testSize, random_state=randomState)\n",
    "\n",
    "  if partitionNumber != 0:\n",
    "    # Obteniendo el total de filas del dataframe de testeo\n",
    "    X_train, X_test, y_train, y_test = X_train.reset_index(drop=True), X_test.reset_index(drop=True), y_train.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "    totalRows = len(X_train)\n",
    "\n",
    "    # Determinando el límite inferior (primera fila) de la partición\n",
    "    bottomLimit = int(totalRows*(1/partitionsNumber)*(partitionNumber-1)) + 1\n",
    "\n",
    "    # Determinando el límite superior (última fila) de la partición\n",
    "    topLimit = int(totalRows*(1/partitionsNumber)*partitionNumber)\n",
    "    #print(bottomLimit, topLimit)\n",
    "\n",
    "    # Determinando una lista con todas los numeros de las filas con la partición\n",
    "    partitionList = [x for x in range(bottomLimit-1, topLimit)]\n",
    "\n",
    "    # Determinando los dataframes de las categorías\n",
    "    X_train, X_test = X_train[~X_train.index.isin(partitionList)], X_train[X_train.index.isin(partitionList)]\n",
    "\n",
    "    # Determinando los dataframes de los objetivos\n",
    "    y_train, y_test = y_train[~y_train.index.isin(partitionList)], y_train[y_train.index.isin(partitionList)]\n",
    "\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "18fae5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createClassifier(modelName):\n",
    "  if modelName == \"KNN\":\n",
    "    return KNeighborsClassifier()\n",
    "  elif modelName == \"LR\":\n",
    "    return LogisticRegression(random_state=randomState, max_iter=200)\n",
    "  elif modelName == \"GNB\":\n",
    "    return GaussianNB()\n",
    "  elif modelName == \"DT\":\n",
    "    return DecisionTreeClassifier(random_state=randomState)\n",
    "  elif modelName == \"SVM\":\n",
    "    return SVC(random_state=randomState)\n",
    "  elif modelName == \"RF\":\n",
    "    return RandomForestClassifier(random_state=randomState)\n",
    "  elif modelName == \"GB\":\n",
    "    return GradientBoostingClassifier(random_state=randomState)\n",
    "  else:\n",
    "    return KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "c2be135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(X_train, X_test, y_train, y_test, modelName):\n",
    "  # Creación del clasificador KNN\n",
    "  clf = createClassifier(modelName)\n",
    "\n",
    "  # Entrenamiento del clasificador KNN\n",
    "  clf.fit(X_train, y_train)\n",
    "\n",
    "  # Calculando la predicción del modelo con la data de prueba\n",
    "  y_pred = clf.predict(X_test)\n",
    "\n",
    "  return y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "e1887988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(y_train, y_test, y_pred, startDate, endDate, partitionNumber, algorithm = \"KNN\"):\n",
    "  trainRows = len(y_train)\n",
    "  testRows = len(y_test)\n",
    "\n",
    "  # Calculando la exactitud del modelo\n",
    "  accuracy = \"{:.2%}\".format(accuracy_score(y_test, y_pred))\n",
    "\n",
    "  # Calculando la precisión del modelo\n",
    "  precision = \"{:.2%}\".format(precision_score(y_test, y_pred))\n",
    "\n",
    "  # Calculando la sensibilidad del modelo\n",
    "  recall = \"{:.2%}\".format(recall_score(y_test, y_pred))\n",
    "\n",
    "  # Calculando el valor F del modelo (robustez)\n",
    "  f1Score = \"{:.2%}\".format(f1_score(y_test, y_pred))\n",
    "\n",
    "  # Calculando el tiempo de ejecución del modelo\n",
    "  executionTime = (endDate - startDate).total_seconds()\n",
    "  formatExecutionTime = \"{:.2f}\".format(executionTime) + \"s\"\n",
    "  formatAverageTime = \"{:.2f}\".format(executionTime*1000/(trainRows + testRows)) + \"ms\"\n",
    "\n",
    "  confussionMatrix = str(confusion_matrix(y_test, y_pred).tolist())\n",
    "  \n",
    "  return {\n",
    "    \"algoritmo\": algorithm,\n",
    "    \"tipo\": \"Total de datos\" if partitionNumber == 0 else \"Particion \" + str(partitionNumber),\n",
    "    \"registrosEntrenamiento\": trainRows,\n",
    "    \"registrosPrueba\": testRows,\n",
    "    \"proporcionSobremuestreo\": samplingStrategy,\n",
    "    \"tiempoEjecucion\": formatExecutionTime,\n",
    "    \"matrizConfusion\": confussionMatrix,\n",
    "    \"exactitud\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"sensibilidad\": recall,\n",
    "    \"valorF\": f1Score,\n",
    "    \"tiempoPromedio\": formatAverageTime\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "83f3c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  # Definiendo el inicio del proceso\n",
    "  startTime = datetime.datetime.now()\n",
    "  print(\"Inicio: \" + str(startTime))\n",
    "  print(\"Se inició el procesamiento\")\n",
    "  print()\n",
    "\n",
    "  # Leyendo la data\n",
    "  data = readData(os.path.join(inputFolder, \"result.csv\"), ',', 'utf-8', 0)\n",
    "\n",
    "  # Determinando los dataframes de las categorías (X) y el objetivo (y)\n",
    "  X, y = preprocessingData(data)\n",
    "\n",
    "  # Creando el arreglo de metricas de cada partición\n",
    "  metricsList = []\n",
    "\n",
    "  # Iterando sobre cada partición\n",
    "  for partitionNumber in range(0,partitionsNumber+1):\n",
    "    # Separando data para el entrenamiento y testeo\n",
    "    X_train, X_test, y_train, y_test = splitData(X, y, partitionNumber)\n",
    "    #print(len(X_train))\n",
    "    #print(len(X_test))\n",
    "\n",
    "    if partitionNumber == 0:\n",
    "      print(\"Total de datos: \")\n",
    "    else:\n",
    "      print(\"Partición {}: \".format(partitionNumber))\n",
    "\n",
    "    models = [\"KNN\", \"LR\", \"GNB\", \"DT\", \"SVM\", \"RF\", \"GB\"]\n",
    "\n",
    "    for model in models:\n",
    "      print(\"Calculando para el algoritmo {}\".format(model))\n",
    "\n",
    "      # Inicio de ejecución del modelo\n",
    "      startDate = datetime.datetime.now()\n",
    "      print(\"Inicio: \" + str(startDate))\n",
    "      \n",
    "      # Realizar entrenamiento del modelo\n",
    "      y_test, y_pred = trainModel(X_train, X_test, y_train, y_test, model)\n",
    "\n",
    "      # Fin de ejecución del modelo\n",
    "      endDate = datetime.datetime.now()\n",
    "      print(\"Fin: \" + str(endDate))\n",
    "      print(\"Tiempo: \" + str(endDate-startDate))\n",
    "      print()\n",
    "\n",
    "      # Obteniendo las métricas de la partición del modelo\n",
    "      metrics = getMetrics(y_train, y_test, y_pred, startDate, endDate, partitionNumber, model)\n",
    "\n",
    "      # Añadiendo la métrica de la partición a la lista de métricas\n",
    "      metricsList.append(metrics)\n",
    "\n",
    "  # Ordenando las métricas\n",
    "  #metricsList = sorted(metricsList, key=lambda x: (x[\"algoritmo\"], x[\"tipo\"]))\n",
    "    \n",
    "  # Escribiendo las metricas en un archivo de salida\n",
    "  outputFileDateTime = datetime.datetime.now()\n",
    "  writeJson(metricsList, os.path.join(outputFolder, \"json\", outputFileDateTime.strftime(\"%Y-%m-%d %H-%M-%S\") + \".json\"))\n",
    "  writeCsv(metricsList, os.path.join(outputFolder, \"csv\", outputFileDateTime.strftime(\"%Y-%m-%d %H-%M-%S\") + \".csv\"))\n",
    "\n",
    "  # Definiendo el fin del proceso\n",
    "  endTime = datetime.datetime.now()\n",
    "  print(\"Fin: \" + str(endTime))\n",
    "  print(\"Tiempo: \" + str(endTime-startTime))\n",
    "  \n",
    "  # Retornando la lista de métricas\n",
    "  return metricsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "7c95c684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio: 2023-05-30 13:24:17.829567\n",
      "Se inició el procesamiento\n",
      "\n",
      "Total de datos: \n",
      "Calculando para el algoritmo KNN\n",
      "Inicio: 2023-05-30 13:24:20.734430\n",
      "Fin: 2023-05-30 13:24:20.936639\n",
      "Tiempo: 0:00:00.202209\n",
      "\n",
      "Calculando para el algoritmo LR\n",
      "Inicio: 2023-05-30 13:24:20.944613\n",
      "Fin: 2023-05-30 13:24:21.368507\n",
      "Tiempo: 0:00:00.423894\n",
      "\n",
      "Calculando para el algoritmo GNB\n",
      "Inicio: 2023-05-30 13:24:21.375440\n",
      "Fin: 2023-05-30 13:24:21.473043\n",
      "Tiempo: 0:00:00.097603\n",
      "\n",
      "Calculando para el algoritmo DT\n",
      "Inicio: 2023-05-30 13:24:21.480019\n",
      "Fin: 2023-05-30 13:24:22.499267\n",
      "Tiempo: 0:00:01.019248\n",
      "\n",
      "Calculando para el algoritmo SVM\n",
      "Inicio: 2023-05-30 13:24:22.506140\n",
      "Fin: 2023-05-30 13:24:33.494687\n",
      "Tiempo: 0:00:10.988547\n",
      "\n",
      "Calculando para el algoritmo RF\n",
      "Inicio: 2023-05-30 13:24:33.500619\n",
      "Fin: 2023-05-30 13:24:36.095049\n",
      "Tiempo: 0:00:02.594430\n",
      "\n",
      "Calculando para el algoritmo GB\n",
      "Inicio: 2023-05-30 13:24:36.101891\n",
      "Fin: 2023-05-30 13:24:40.884248\n",
      "Tiempo: 0:00:04.782357\n",
      "\n",
      "Partición 1: \n",
      "Calculando para el algoritmo KNN\n",
      "Inicio: 2023-05-30 13:24:40.942954\n",
      "Fin: 2023-05-30 13:24:41.088897\n",
      "Tiempo: 0:00:00.145943\n",
      "\n",
      "Calculando para el algoritmo LR\n",
      "Inicio: 2023-05-30 13:24:41.095874\n",
      "Fin: 2023-05-30 13:24:41.351180\n",
      "Tiempo: 0:00:00.255306\n",
      "\n",
      "Calculando para el algoritmo GNB\n",
      "Inicio: 2023-05-30 13:24:41.356162\n",
      "Fin: 2023-05-30 13:24:41.429112\n",
      "Tiempo: 0:00:00.072950\n",
      "\n",
      "Calculando para el algoritmo DT\n",
      "Inicio: 2023-05-30 13:24:41.436085\n",
      "Fin: 2023-05-30 13:24:42.192801\n",
      "Tiempo: 0:00:00.756716\n",
      "\n",
      "Calculando para el algoritmo SVM\n",
      "Inicio: 2023-05-30 13:24:42.198781\n",
      "Fin: 2023-05-30 13:24:48.227098\n",
      "Tiempo: 0:00:06.028317\n",
      "\n",
      "Calculando para el algoritmo RF\n",
      "Inicio: 2023-05-30 13:24:48.233080\n",
      "Fin: 2023-05-30 13:24:50.086954\n",
      "Tiempo: 0:00:01.853874\n",
      "\n",
      "Calculando para el algoritmo GB\n",
      "Inicio: 2023-05-30 13:24:50.092935\n",
      "Fin: 2023-05-30 13:24:53.725963\n",
      "Tiempo: 0:00:03.633028\n",
      "\n",
      "Partición 2: \n",
      "Calculando para el algoritmo KNN\n",
      "Inicio: 2023-05-30 13:24:53.783512\n",
      "Fin: 2023-05-30 13:24:53.941987\n",
      "Tiempo: 0:00:00.158475\n",
      "\n",
      "Calculando para el algoritmo LR\n",
      "Inicio: 2023-05-30 13:24:53.949960\n",
      "Fin: 2023-05-30 13:24:54.221412\n",
      "Tiempo: 0:00:00.271452\n",
      "\n",
      "Calculando para el algoritmo GNB\n",
      "Inicio: 2023-05-30 13:24:54.227391\n",
      "Fin: 2023-05-30 13:24:54.296006\n",
      "Tiempo: 0:00:00.068615\n",
      "\n",
      "Calculando para el algoritmo DT\n",
      "Inicio: 2023-05-30 13:24:54.301986\n",
      "Fin: 2023-05-30 13:24:55.046743\n",
      "Tiempo: 0:00:00.744757\n",
      "\n",
      "Calculando para el algoritmo SVM\n",
      "Inicio: 2023-05-30 13:24:55.053828\n",
      "Fin: 2023-05-30 13:25:01.415826\n",
      "Tiempo: 0:00:06.361998\n",
      "\n",
      "Calculando para el algoritmo RF\n",
      "Inicio: 2023-05-30 13:25:01.421696\n",
      "Fin: 2023-05-30 13:25:03.391763\n",
      "Tiempo: 0:00:01.970067\n",
      "\n",
      "Calculando para el algoritmo GB\n",
      "Inicio: 2023-05-30 13:25:03.397743\n",
      "Fin: 2023-05-30 13:25:06.971807\n",
      "Tiempo: 0:00:03.574064\n",
      "\n",
      "Partición 3: \n",
      "Calculando para el algoritmo KNN\n",
      "Inicio: 2023-05-30 13:25:07.030442\n",
      "Fin: 2023-05-30 13:25:07.184783\n",
      "Tiempo: 0:00:00.154341\n",
      "\n",
      "Calculando para el algoritmo LR\n",
      "Inicio: 2023-05-30 13:25:07.191760\n",
      "Fin: 2023-05-30 13:25:07.452217\n",
      "Tiempo: 0:00:00.260457\n",
      "\n",
      "Calculando para el algoritmo GNB\n",
      "Inicio: 2023-05-30 13:25:07.457197\n",
      "Fin: 2023-05-30 13:25:07.527965\n",
      "Tiempo: 0:00:00.070768\n",
      "\n",
      "Calculando para el algoritmo DT\n",
      "Inicio: 2023-05-30 13:25:07.532948\n",
      "Fin: 2023-05-30 13:25:08.142838\n",
      "Tiempo: 0:00:00.609890\n",
      "\n",
      "Calculando para el algoritmo SVM\n",
      "Inicio: 2023-05-30 13:25:08.148818\n",
      "Fin: 2023-05-30 13:25:14.962847\n",
      "Tiempo: 0:00:06.814029\n",
      "\n",
      "Calculando para el algoritmo RF\n",
      "Inicio: 2023-05-30 13:25:14.968828\n",
      "Fin: 2023-05-30 13:25:16.891938\n",
      "Tiempo: 0:00:01.923110\n",
      "\n",
      "Calculando para el algoritmo GB\n",
      "Inicio: 2023-05-30 13:25:16.897919\n",
      "Fin: 2023-05-30 13:25:20.497717\n",
      "Tiempo: 0:00:03.599798\n",
      "\n",
      "Partición 4: \n",
      "Calculando para el algoritmo KNN\n",
      "Inicio: 2023-05-30 13:25:20.554884\n",
      "Fin: 2023-05-30 13:25:20.702914\n",
      "Tiempo: 0:00:00.148030\n",
      "\n",
      "Calculando para el algoritmo LR\n",
      "Inicio: 2023-05-30 13:25:20.709395\n",
      "Fin: 2023-05-30 13:25:21.008551\n",
      "Tiempo: 0:00:00.299156\n",
      "\n",
      "Calculando para el algoritmo GNB\n",
      "Inicio: 2023-05-30 13:25:21.013438\n",
      "Fin: 2023-05-30 13:25:21.092267\n",
      "Tiempo: 0:00:00.078829\n",
      "\n",
      "Calculando para el algoritmo DT\n",
      "Inicio: 2023-05-30 13:25:21.098247\n",
      "Fin: 2023-05-30 13:25:21.649077\n",
      "Tiempo: 0:00:00.550830\n",
      "\n",
      "Calculando para el algoritmo SVM\n",
      "Inicio: 2023-05-30 13:25:21.654983\n",
      "Fin: 2023-05-30 13:25:27.734099\n",
      "Tiempo: 0:00:06.079116\n",
      "\n",
      "Calculando para el algoritmo RF\n",
      "Inicio: 2023-05-30 13:25:27.739082\n",
      "Fin: 2023-05-30 13:25:29.605726\n",
      "Tiempo: 0:00:01.866644\n",
      "\n",
      "Calculando para el algoritmo GB\n",
      "Inicio: 2023-05-30 13:25:29.611708\n",
      "Fin: 2023-05-30 13:25:33.207692\n",
      "Tiempo: 0:00:03.595984\n",
      "\n",
      "Fin: 2023-05-30 13:25:33.214671\n",
      "Tiempo: 0:01:15.385104\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
